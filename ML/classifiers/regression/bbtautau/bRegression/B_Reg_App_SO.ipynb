{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B regressor final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goncalo/anaconda2/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division ,print_function\n",
    "\n",
    "import theano\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, GaussianNoise, BatchNormalization, Merge, GaussianDropout\n",
    "from keras.layers.advanced_activations import ELU, PReLU\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "import theano.tensor as T\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy.misc\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "import sys\n",
    "sys.path.append('../../../../modules')\n",
    "from MPPlot import *\n",
    "from Processors import *\n",
    "#import warnings\n",
    "#with warnings.catch_warnings():\n",
    "#    warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "Here just looking at mu tau_h b b final-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples contains 9762 signal events and 168053 background events\n",
      "177815 events in total\n"
     ]
    }
   ],
   "source": [
    "mode = \"mu_tau_b_b\"\n",
    "loc = '../../../../../data_n/'\n",
    "SignalData = []\n",
    "BackgroundData = []\n",
    "\n",
    "energies = [260, 270, 280, 300, 320, 500, 550, 600 ,700]\n",
    "backgrounds = ['TT' , 'WJetsToLNu']\n",
    "Data = np.concatenate((energies,backgrounds))\n",
    "\n",
    "def backgroundName(background):\n",
    "    if background == 'TT':\n",
    "        return loc + 'TT_TuneCUETP8M1_13TeV-powheg-pythia8_2.csv'\n",
    "    elif background == 'WJetsToLNu':\n",
    "        return loc + 'WJetsToLNu_TuneCUETP8M1_13TeV-amcatnloFXFX-pythia8.csv'\n",
    "\n",
    "\n",
    "\n",
    " #Load data from CSV files\n",
    "for energy in energies:\n",
    "    signaldata = pandas.read_csv(loc + \"GluGluToRadionToHHTo2B2Tau_M-\" + str(energy) +\"_narrow_13TeV-madgraph.csv\")\n",
    "    signaldata['gen_label'] = energy\n",
    "    SignalData.append(signaldata)\n",
    "\n",
    "\n",
    "for back in backgrounds:\n",
    "    backgroundData = pandas.read_csv(backgroundName(back))\n",
    "    backgroundData['gen_label'] = back\n",
    "    BackgroundData.append(backgroundData)    \n",
    "     \n",
    "    \n",
    "signalData = SignalData[0]\n",
    "for signal_data in SignalData[1:]:\n",
    "    signalData = signalData.append(signal_data, ignore_index=True)    \n",
    "signalData.drop([x for x in signalData.columns if 'Unnamed' in x], axis=1, inplace=True)    \n",
    "signalData['gen_target'] = pandas.Series(np.ones(signalData.size))\n",
    "backgroundData = BackgroundData[0]\n",
    "for background_data in BackgroundData[1:]:\n",
    "    backgroundData = backgroundData.append(background_data, ignore_index=True)    \n",
    "backgroundData.drop([x for x in backgroundData.columns if 'Unnamed' in x], axis=1, inplace=True)    \n",
    "backgroundData['gen_target'] = pandas.Series(np.zeros(backgroundData.size))\n",
    "\n",
    "print(\"Samples contains {0} signal events and {1} background events\".format(len(signalData), len(backgroundData)))\n",
    "print(\"{} events in total\".format(len(signalData)+len(backgroundData)))\n",
    "data = signalData.append(backgroundData, ignore_index = True) #Combine into signal dataset\n",
    "\n",
    "\n",
    "def abs_(x):\n",
    "    if type(x) is float:\n",
    "        return abs(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def cleanData(X):\n",
    "    \"\"\"\n",
    "    Recives data and labels (X and y)\n",
    "    \n",
    "    Returns: X, y after removing points that would fail to convert to float32 \n",
    "    \"\"\"\n",
    "    X.applymap(abs_)\n",
    "    \n",
    "    \n",
    "    over  = (X > np.finfo(np.float32).max)\n",
    "    under = (X < np.finfo(np.float32).min)\n",
    "    selecting = pandas.Series(np.zeros(len(X)), dtype=np.bool)\n",
    "\n",
    "    for label in over.columns:\n",
    "        if label != 'gen_label':\n",
    "            selecting = selecting | over[label] | under[label]\n",
    "    \n",
    "    \n",
    "    X = X[np.logical_not(selecting)].reset_index(drop=True)#.values.astype('float32')\n",
    "    \n",
    "    return X\n",
    "\n",
    "data = cleanData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create development and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sig_devIndeces, sig_valIndeces = \\\n",
    "                train_test_split([i for i in data[(data.gen_target == 1) & (data.gen_mctMatch == 1)].index.tolist()],\n",
    "                                 test_size=0.2, random_state=1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move to cartesian coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "particles = ['t_0', 't_1', 'b_0', 'b_1', 'h_tt', 'h_bb', 'diH', 'gen_t_0', 'gen_t_1', 'gen_b_0' ,'gen_b_1']\n",
    "\n",
    "for p in particles:\n",
    "    moveToCartesian(data, p) #Move pT, eta, and phi to p_x, p_y, and p_z\n",
    "    if(not str.startswith(p, \"gen\")):\n",
    "        addEnergy(data, p) #Calculate energy and absolute momentum\n",
    "\n",
    "\n",
    "moveToCartesian(data, 'mPT', False)  #Move Missing pT and phi to p_x and p_y\n",
    "addAbsMom(data, 'mPT', False) #Calculate absolute missing transverse momentum\n",
    "addMT(data, data['t_1_pT'], data['t_1_phi'], 't_1') #Calculate transverse mass of tau_mu\n",
    "addHighLvl(data)\n",
    "data['hl_mT'] = np.sqrt(2*data['t_1_pT']*data['mPT_pT']*(1-np.cos(deltaphi(data['t_1_phi'], data['mPT_phi']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "genFeatures = [gen for gen in data.columns if str.startswith(gen, \"gen\")]\n",
    "trainFeatures = [var for var in data.columns if var not in genFeatures]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_0_mass', 't_1_mass', 'b_0_mass', 'b_0_csv', 'b_1_mass', 'b_1_csv', 'h_tt_mass', 'h_tt_svFit_mass', 'h_bb_mass', 'diH_mass', 'diH_kinFit_mass', 'mT', 'hT', 'sT', 'centrality', 'eVis', 'sphericity', 'spherocity', 'aplanarity', 'aplanority', 'upsilon', 'dShape', 'sphericityEigen0', 'sphericityEigen1', 'sphericityEigen2', 'spherocityEigen0', 'spherocityEigen1', 'spherocityEigen2', 't_0_px', 't_0_py', 't_0_pz', 't_0_|p|', 't_0_E', 't_1_px', 't_1_py', 't_1_pz', 't_1_|p|', 't_1_E', 'b_0_px', 'b_0_py', 'b_0_pz', 'b_0_|p|', 'b_0_E', 'b_1_px', 'b_1_py', 'b_1_pz', 'b_1_|p|', 'b_1_E', 'h_tt_px', 'h_tt_py', 'h_tt_pz', 'h_tt_|p|', 'h_tt_E', 'h_bb_px', 'h_bb_py', 'h_bb_pz', 'h_bb_|p|', 'h_bb_E', 'diH_px', 'diH_py', 'diH_pz', 'diH_|p|', 'diH_E', 'mPT_px', 'mPT_py', 'mPT_|p|', 't_1_mT', 'hl_dpx_b_0_b_1', 'hl_dpy_b_0_b_1', 'hl_dpz_b_0_b_1', 'hl_dpx_b_0_t_0', 'hl_dpy_b_0_t_0', 'hl_dpz_b_0_t_0', 'hl_dpx_b_0_t_1', 'hl_dpy_b_0_t_1', 'hl_dpz_b_0_t_1', 'hl_dpx_b_0_mPT', 'hl_dpy_b_0_mPT', 'hl_dpx_b_1_b_0', 'hl_dpy_b_1_b_0', 'hl_dpz_b_1_b_0', 'hl_dpx_b_1_t_0', 'hl_dpy_b_1_t_0', 'hl_dpz_b_1_t_0', 'hl_dpx_b_1_t_1', 'hl_dpy_b_1_t_1', 'hl_dpz_b_1_t_1', 'hl_dpx_b_1_mPT', 'hl_dpy_b_1_mPT', 'hl_dpx_t_0_b_0', 'hl_dpy_t_0_b_0', 'hl_dpz_t_0_b_0', 'hl_dpx_t_0_b_1', 'hl_dpy_t_0_b_1', 'hl_dpz_t_0_b_1', 'hl_dpx_t_0_t_1', 'hl_dpy_t_0_t_1', 'hl_dpz_t_0_t_1', 'hl_dpx_t_0_mPT', 'hl_dpy_t_0_mPT', 'hl_dpx_t_1_b_0', 'hl_dpy_t_1_b_0', 'hl_dpz_t_1_b_0', 'hl_dpx_t_1_b_1', 'hl_dpy_t_1_b_1', 'hl_dpz_t_1_b_1', 'hl_dpx_t_1_t_0', 'hl_dpy_t_1_t_0', 'hl_dpz_t_1_t_0', 'hl_dpx_t_1_mPT', 'hl_dpy_t_1_mPT', 'hl_dpx_diH_h_bb', 'hl_dpy_diH_h_bb', 'hl_dpz_diH_h_bb', 'hl_dpx_diH_h_tt', 'hl_dpy_diH_h_tt', 'hl_dpz_diH_h_tt', 'hl_dpx_diH_mPT', 'hl_dpy_diH_mPT', 'hl_dpx_h_bb_diH', 'hl_dpy_h_bb_diH', 'hl_dpz_h_bb_diH', 'hl_dpx_h_bb_h_tt', 'hl_dpy_h_bb_h_tt', 'hl_dpz_h_bb_h_tt', 'hl_dpx_h_bb_mPT', 'hl_dpy_h_bb_mPT', 'hl_dpx_h_tt_diH', 'hl_dpy_h_tt_diH', 'hl_dpz_h_tt_diH', 'hl_dpx_h_tt_h_bb', 'hl_dpy_h_tt_h_bb', 'hl_dpz_h_tt_h_bb', 'hl_dpx_h_tt_mPT', 'hl_dpy_h_tt_mPT', 'hl_mT']\n"
     ]
    }
   ],
   "source": [
    "pTEtaPhi = [var for var in trainFeatures for x in ['pT', 'eta', 'phi'] if x in var]\n",
    "\n",
    "other = [var for var in trainFeatures for x in ['prob'] if x in var]\n",
    "trainFeatures = [var for var in trainFeatures if var not in pTEtaPhi+ other]\n",
    "print(trainFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shapes = [var for var in trainFeatures for x in ['aplan', 'dShape', 'spher', 'upsilon'] if x in var]\n",
    "shapeFeatures = [var for var in trainFeatures if var in shapes]\n",
    "eventKinematicFeatures = ['centrality', 'eVis', 'hT', 'sT','mT']\n",
    "jetFeatures = [var for var in trainFeatures if 'Jet' in var and 'Jets' not in var]\n",
    "multiplicityFeatures = ['nBJets', 'nJets', 'nPhotons', 'nTauJets']\n",
    "hlFeatures = [var for var in trainFeatures if (str.startswith(var, \"hl_\"))]\n",
    "b1Features = [var for var in trainFeatures if (str.startswith(var, \"b_1\"))]\n",
    "recoFeatures = [var for var in trainFeatures if (str.startswith(var, \"h_\")) or (str.startswith(var, \"diH_\"))]\n",
    "epFeatures = [var for var in trainFeatures if (str.endswith(var, \"_E\")) or (str.endswith(var, \"_|p|\"))]\n",
    "fsFeatures =  [var for var in trainFeatures if var not in pTEtaPhi + recoFeatures + hlFeatures + b1Features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set0 = [var for var in fsFeatures if var not in epFeatures]   \n",
    "set1 = [var for var in fsFeatures]  \n",
    "set2 = [var for var in fsFeatures + recoFeatures if var not in pTEtaPhi + epFeatures] + ['hl_mT']\n",
    "set3 = [var for var in fsFeatures + recoFeatures if var not in pTEtaPhi] + ['hl_mT']\n",
    "set4 = ['b_0_mass', 'b_0_px', 'b_0_py', 'b_0_pz', 'b_0_|p|', 'b_0_E',\n",
    "        'mPT_px', 'mPT_py',\n",
    "        'h_bb_E',\n",
    "        't_0_mass', 't_0_px', 't_0_py', 't_0_pz', 't_0_|p|', 't_0_E',\n",
    "        't_1_mass', 't_1_px', 't_1_py', 't_1_pz', 't_1_|p|', 't_1_E',\n",
    "        'hl_mT',\n",
    "        'h_tt_mass', 'h_tt_px', 'h_tt_py', 'h_tt_pz', 'h_tt_|p|', 'h_tt_E',\n",
    "        'diH_E', 'diH_|p|', 'diH_mass']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regTrainFeatures = list(set4)\n",
    "regModel = 'model0'\n",
    "normIn = True\n",
    "normOut = True\n",
    "pca = True\n",
    "whiten = False\n",
    "nSplits = 10\n",
    "cvTests = True\n",
    "ensembleSize = 5\n",
    "ensembleMode = 'loss'\n",
    "regTargetFeatures = ['gen_b_0_px', 'gen_b_0_py', 'gen_b_0_pz']\n",
    "print (\"\\nTraining on\", len(regTrainFeatures), \"features:\", [var for var in regTrainFeatures])\n",
    "print (\"\\nRegressing to\", len(regTargetFeatures), \"features:\", [var for var in regTargetFeatures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3705 events for training, 174101 events for validation\n"
     ]
    }
   ],
   "source": [
    "devData = data.loc[sig_devIndeces].copy()\n",
    "#devData = devData.append(data.loc[bkg_devIndeces].copy(), ignore_index = True)\n",
    "valData = data.loc[sig_valIndeces].copy()\n",
    "valData = valData.append(data[data.gen_target == 0].copy(), ignore_index = True)\n",
    "valData = valData.append(data[(data.gen_target == 1) & (data.gen_mctMatch == 0)].copy(), ignore_index = True)\n",
    "sig = (valData.gen_target == 1) & (valData.gen_mctMatch == 1)\n",
    "bkg = (valData.gen_target == 0)\n",
    "sigMM = (valData.gen_target == 1) & (valData.gen_mctMatch == 0)\n",
    "\n",
    "print(\"{} events for training, {} events for validation\".format(len(devData), len(valData)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gen_b_0_px', 'gen_b_0_py', 'gen_b_0_pz']\n",
      "['gen_b_1_px', 'gen_b_1_py', 'gen_b_1_pz']\n"
     ]
    }
   ],
   "source": [
    "regTrainFeatures0 = list(regTrainFeatures)\n",
    "regTrainFeatures1 = list(regTrainFeatures)\n",
    "for i in range(len(regTrainFeatures1)):\n",
    "    if \"b_0\" in regTrainFeatures1[i]:\n",
    "        regTrainFeatures1[i] = regTrainFeatures1[i][0:regTrainFeatures1[i].find(\"b_0\")] + \"b_1\" + regTrainFeatures1[i][regTrainFeatures1[i].find(\"b_0\")+3:]\n",
    "regTargetFeatures0 = list(regTargetFeatures)\n",
    "regTargetFeatures1 = list(regTargetFeatures)\n",
    "for i in range(len(regTargetFeatures1)):\n",
    "    if \"b_0\" in regTargetFeatures1[i]:\n",
    "        regTargetFeatures1[i] = regTargetFeatures1[i][0:regTargetFeatures1[i].find(\"b_0\")] + \"b_1\" + regTargetFeatures1[i][regTargetFeatures1[i].find(\"b_0\")+3:]\n",
    "print(regTargetFeatures0)\n",
    "print(regTargetFeatures1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_reg_0 = devData[devData.gen_target == 1][regTrainFeatures0].values.astype(theano.config.floatX)         \n",
    "X_reg_1 = devData[devData.gen_target == 1][regTrainFeatures1].values.astype(theano.config.floatX)\n",
    "y_reg_0 = devData[devData.gen_target == 1][regTargetFeatures0].values.astype(theano.config.floatX)\n",
    "y_reg_1 = devData[devData.gen_target == 1][regTargetFeatures1].values.astype(theano.config.floatX)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "compileArgs = [None,None]\n",
    "compileArgs[0] = {'loss' : 'mse', 'optimizer' : 'nadam'}\n",
    "compileArgs[1] = {'loss' : 'mse', 'optimizer' : 'nadam'}\n",
    "def getRegressor(j):\n",
    "    if(j == 0):\n",
    "        regModel = Sequential()\n",
    "\n",
    "        regModel.add(Dense(50, input_dim=len(regTrainFeatures), kernel_initializer='he_normal', \n",
    "                           activation='selu'))\n",
    "        regModel.add(GaussianNoise(1))\n",
    "        #regModel.add(GaussianDropout(0.3))\n",
    "        regModel.add(BatchNormalization())\n",
    "        regModel.add(Dropout(0.1))\n",
    "        regModel.add(Dense(50, kernel_initializer='he_normal', \n",
    "                           activation='selu'))\n",
    "        regModel.add(GaussianNoise(1))\n",
    "        regModel.add(BatchNormalization())\n",
    "        #regModel.add(GaussianDropout(0.3))\n",
    "        regModel.add(Dropout(0.1))\n",
    "        regModel.add(Dense(60, kernel_initializer='he_normal', \n",
    "                           activation='selu'))\n",
    "        regModel.add(GaussianNoise(1))\n",
    "        regModel.add(BatchNormalization())\n",
    "        #regModel.add(GaussianDropout(0.3))\n",
    "        regModel.add(Dropout(0.1))\n",
    "        regModel.add(Dense(70, kernel_initializer='he_normal', \n",
    "                           activation='selu'))\n",
    "        regModel.add(GaussianNoise(1))\n",
    "        regModel.add(BatchNormalization())\n",
    "        #regModel.add(GaussianDropout(0.3))\n",
    "        regModel.add(Dropout(0.1))\n",
    "        #regModel.add(Dense(200, kernel_initializer='he_normal', \n",
    "        #                   activation='selu'))\n",
    "        #regModel.add(GaussianNoise(1))\n",
    "        #regModel.add(BatchNormalization())\n",
    "        #regModel.add(GaussianDropout(0.3))\n",
    "        #regModel.add(Dropout(0.2))\n",
    "    if(j == 1):\n",
    "        regModel = Sequential()\n",
    "\n",
    "        regModel.add(Dense(70, input_dim=len(regTrainFeatures), kernel_initializer='he_normal', \n",
    "                           activation='selu'))\n",
    "        regModel.add(GaussianNoise(1))\n",
    "        #regModel.add(GaussianDropout(0.3))\n",
    "        regModel.add(BatchNormalization())\n",
    "        regModel.add(Dropout(0.1))\n",
    "        regModel.add(Dense(70, kernel_initializer='he_normal', \n",
    "                           activation='selu'))\n",
    "        regModel.add(GaussianNoise(1))\n",
    "        regModel.add(BatchNormalization())\n",
    "        #regModel.add(GaussianDropout(0.3))\n",
    "        regModel.add(Dropout(0.1))\n",
    "        regModel.add(Dense(70, kernel_initializer='he_normal', \n",
    "                           activation='selu'))\n",
    "        regModel.add(GaussianNoise(1))\n",
    "        regModel.add(BatchNormalization())\n",
    "        #regModel.add(GaussianDropout(0.3))\n",
    "        regModel.add(Dropout(0.3))\n",
    "        regModel.add(Dense(70, kernel_initializer='he_normal', \n",
    "                           activation='selu'))\n",
    "        regModel.add(GaussianNoise(1))\n",
    "        regModel.add(BatchNormalization())\n",
    "        #regModel.add(GaussianDropout(0.3))\n",
    "        regModel.add(Dropout(0.3))\n",
    "        #regModel.add(Dense(100, kernel_initializer='he_normal', \n",
    "        #                   activation='selu'))\n",
    "        #regModel.add(GaussianNoise(1))\n",
    "        #regModel.add(BatchNormalization())\n",
    "        #regModel.add(GaussianDropout(0.3))\n",
    "        #regModel.add(Dropout(0.2))\n",
    "    regModel.add(Dense(len(regTargetFeatures), activation='linear', kernel_initializer='he_normal'))\n",
    "    regModel.compile(**compileArgs[j])\n",
    "    return regModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate invariant mass of b pair and pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMass(pred):\n",
    "    tmpData = pandas.DataFrame(np.nan, index=[x for x in range(len(pred))], columns=['ID'])\n",
    "    for n, feature in enumerate(regTargetFeatures0):\n",
    "        tmpData['reg_' + feature[4:]] = pandas.Series(pred[:,n], index=tmpData.index)\n",
    "    for j, feature in enumerate(regTargetFeatures1):\n",
    "        tmpData['reg_' + feature[4:]] = pandas.Series(pred[:,n+j], index=tmpData.index)\n",
    "    bMass = 4.8\n",
    "    tmpData['reg_b_0_E'] = np.sqrt((bMass**2)+np.square(tmpData.loc[:,'reg_b_0_px'])+np.square(tmpData.loc[:,'reg_b_0_py'])+np.square(tmpData.loc[:,'reg_b_0_pz']))\n",
    "    tmpData['reg_b_1_E'] = np.sqrt((bMass**2)+np.square(tmpData.loc[:,'reg_b_1_px'])+np.square(tmpData.loc[:,'reg_b_1_py'])+np.square(tmpData.loc[:,'reg_b_1_pz']))\n",
    "    tmpData['reg_h_bb_px'] = tmpData.loc[:,'reg_b_0_px']+tmpData.loc[:,'reg_b_1_px']\n",
    "    tmpData['reg_h_bb_py'] = tmpData.loc[:,'reg_b_0_py']+tmpData.loc[:,'reg_b_1_py']\n",
    "    tmpData['reg_h_bb_pz'] = tmpData.loc[:,'reg_b_0_pz']+tmpData.loc[:,'reg_b_1_pz']\n",
    "    tmpData['reg_h_bb_E'] = tmpData.loc[:,'reg_b_0_E']+tmpData.loc[:,'reg_b_1_E']\n",
    "    tmpData['reg_h_bb_p2'] = np.square(tmpData.loc[:,'reg_h_bb_px'])+np.square(tmpData.loc[:,'reg_h_bb_py'])+np.square(tmpData.loc[:,'reg_h_bb_pz'])\n",
    "    tmpData['reg_h_bb_mass'] = np.sqrt(np.square(tmpData.loc[:,'reg_h_bb_E'])-tmpData.loc[:,'reg_h_bb_p2'])\n",
    "    return tmpData['reg_h_bb_mass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  gen_b_1_px\n",
      "1  gen_b_1_py\n",
      "2  gen_b_1_pz\n"
     ]
    }
   ],
   "source": [
    "for n, feature in enumerate(regTargetFeatures1):\n",
    "        print( n , \"\" ,feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getResponse(pred):\n",
    "    masses = calcMass(pred)\n",
    "    pull = 125-masses.values\n",
    "    return (abs(pull.mean()), pull.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputPipe = [None,None]\n",
    "outputPipe = [None,None]\n",
    "for j in range(0,2):\n",
    "    stepsIn = []\n",
    "    if not normIn and not pca:\n",
    "        stepsIn.append(('ident', StandardScaler(with_mean=False, with_std=False))) #For compatability\n",
    "    else:\n",
    "        if normIn:\n",
    "            stepsIn.append(('normIn', StandardScaler()))\n",
    "        if pca:\n",
    "            stepsIn.append(('pca', PCA(whiten=whiten)))\n",
    "    inputPipe[j]=(Pipeline(stepsIn))\n",
    "    stepsOut = []\n",
    "    if normOut:\n",
    "        stepsOut.append(('normOut', StandardScaler()))\n",
    "    else:\n",
    "        stepsOut.append(('ident', StandardScaler(with_mean=False, with_std=False))) #For compatability\n",
    "    outputPipe[j]=(Pipeline(stepsOut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_reg_0 = inputPipe[0].fit_transform(X_reg_0)\n",
    "y_reg_0 = outputPipe[0].fit_transform(y_reg_0)\n",
    "X_reg_1 = inputPipe[1].fit_transform(X_reg_1)\n",
    "y_reg_1 = outputPipe[1].fit_transform(y_reg_1)\n",
    "#X_reg = inputPipe.fit_transform(devData[regTrainFeatures].values.astype(theano.config.floatX))\n",
    "#y_reg = outputPipe.fit_transform(devData[regTargetFeatures].values.astype(theano.config.floatX))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train regressors\n",
    "Train nSplit times to find best convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData = (None, None)\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {}\n",
    "        self.losses['loss'] = []\n",
    "        self.losses['val_loss'] = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses['loss'].append(self.model.evaluate(trainingData[0], trainingData[1], verbose=0))\n",
    "        self.losses['val_loss'].append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainParams = {'epochs' : 10000, 'batch_size' : 64, 'verbose' : 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 / 10\n",
      "Epoch 00030: early stopping\n",
      "Score is: {'loss': 0.032591267049593746}\n",
      "Running fold 2 / 10\n",
      "Epoch 00023: early stopping\n",
      "Score is: {'loss': 0.042749060573924906}\n",
      "Running fold 3 / 10\n",
      "Epoch 00026: early stopping\n",
      "Score is: {'loss': 0.045561386362682137}\n",
      "Running fold 4 / 10\n",
      "Epoch 00032: early stopping\n",
      "Score is: {'loss': 0.036192415558263621}\n",
      "Running fold 5 / 10\n",
      "Epoch 00027: early stopping\n",
      "Score is: {'loss': 0.0391054672372309}\n",
      "Running fold 6 / 10\n",
      "Epoch 00028: early stopping\n",
      "Score is: {'loss': 0.067853212155200335}\n",
      "Running fold 7 / 10\n",
      "Epoch 00024: early stopping\n",
      "Score is: {'loss': 0.043422904650907257}\n",
      "Running fold 8 / 10\n",
      "Epoch 00028: early stopping\n",
      "Score is: {'loss': 0.039273152349365723}\n",
      "Running fold 9 / 10\n",
      "Epoch 00033: early stopping\n",
      "Score is: {'loss': 0.07212380354066153}\n",
      "Running fold 10 / 10\n",
      "Epoch 00027: early stopping\n",
      "Score is: {'loss': 0.032362570573349257}\n",
      "Running fold 1 / 10\n",
      "Epoch 00032: early stopping\n",
      "Score is: {'loss': 0.068852923695008061}\n",
      "Running fold 2 / 10\n",
      "Epoch 00026: early stopping\n",
      "Score is: {'loss': 0.071158306517530329}\n",
      "Running fold 3 / 10\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "histories = []\n",
    "os.system(\"rm train_weights/*.h5\")\n",
    "os.system(\"rm train_weights/*.json\")\n",
    "os.system(\"rm train_weights/*.pkl\")\n",
    "\n",
    "X_reg=[X_reg_0,X_reg_1]\n",
    "y_reg=[y_reg_0,y_reg_1]\n",
    "for j in range(0,2):\n",
    "    results = []\n",
    "    \n",
    "    if cvTests:\n",
    "        kf = KFold(n_splits=nSplits, shuffle=True)\n",
    "        i = 0\n",
    "        for train, test in kf.split(X_reg[j]):\n",
    "            i += 1\n",
    "            print (\"Running fold\", i, \"/\", nSplits)\n",
    "            model = None # Clearing the NN\n",
    "            model = getRegressor(j)\n",
    "            model.reset_states #Just checking\n",
    "            trainingData = (X_reg[j][train], y_reg[j][train])\n",
    "            lossHistory = LossHistory()\n",
    "            earlyStop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto', min_delta=0.01)\n",
    "            saveBest = ModelCheckpoint(\"train_weights/best.h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "            model.fit(X_reg[j][train], y_reg[j][train],\n",
    "                      validation_data = (X_reg[j][test], y_reg[j][test]),\n",
    "                      callbacks = [earlyStop, saveBest, lossHistory],\n",
    "                      **trainParams)\n",
    "            histories.append(lossHistory.losses)\n",
    "            model.load_weights(\"train_weights/best.h5\")\n",
    "            results.append({})\n",
    "           # results[-1]['mean'], results[-1]['std'] = getResponse(outputPipe.inverse_transform(model.predict(X_reg[test], verbose=0)))\n",
    "            results[-1]['loss'] = model.evaluate(X_reg[j][test], y_reg[j][test], verbose=0)\n",
    "            print (\"Score is:\", results[-1])\n",
    "            model.save('train_weights/train_' + str(i-1)+ '_'+ str(j) + '.h5')\n",
    "    with open('train_weights/resultsFile' + '_' + str(j) + '.pkl', 'w') as fout:\n",
    "        pickle.dump(results, fout)\n",
    "\n",
    "print (\"Cross-validation took {:.3f}s \".format(time.time() - start))\n",
    "X_reg = None\n",
    "y_reg = None\n",
    "train = None\n",
    "test = None\n",
    "model.summary()\n",
    "model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "if cvTests:\n",
    "    for i, history in enumerate(histories):\n",
    "        if i == 0:\n",
    "            plt.plot(history['loss'], color='g', label='Training')\n",
    "            plt.plot(history['val_loss'], color='b', label='Testing')\n",
    "        else:\n",
    "            plt.plot(history['loss'], color='g')\n",
    "            plt.plot(history['val_loss'], color='b')\n",
    "    plt.legend(fontsize=16)\n",
    "else:\n",
    "    for history in histories:\n",
    "        plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"Epoch\", fontsize=24, color='black')\n",
    "plt.ylabel(\"MSE\", fontsize=24, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aresults = []\n",
    "for j in range (0,2):\n",
    "    with open('train_weights/resultsFile' + '_' + str(j) + '.pkl', 'r') as fin:   \n",
    "        aresults.append(pickle.load(fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadModel(cycle, j, location='train_weights/train_'):\n",
    "    cycle = int(cycle)\n",
    "    model = load_model(location + str(cycle) + '_'+ str(j) +'.h5')\n",
    "    model.compile(**compileArgs[j])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWeights(value, met):\n",
    "    return 1/value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = [[],[]]\n",
    "ensemble = [[],[]]\n",
    "print (\"Choosing ensemble by\", ensembleMode)\n",
    "for j in range(0,2):\n",
    "    #weights=[]\n",
    "    #ensemble = []\n",
    "    if ensembleMode == 'mix':\n",
    "        for met in aresults[j][0]:\n",
    "            print (\"Choosing best for metric\", met)\n",
    "            cycle = np.argmin([aresult[j][met] for result in aresults[j]])\n",
    "            print (met, \"Model\", \"is\", cycle, \"with\", met, \"=\", aresults[j][cycle][met])\n",
    "            ensemble[j].append(loadModel(cycle) , j)\n",
    "            weights[j].append(1)\n",
    "    else:\n",
    "        dtype = [('cycle', int), ('result', float)]\n",
    "        values = np.sort(np.array([(i, result[ensembleMode]) for i, result in enumerate(aresults[j])], dtype=dtype),\n",
    "                         order=['result'])\n",
    "        for i in range(min([ensembleSize, len(aresults[j])])):\n",
    "            ensemble[j].append(loadModel(values[i]['cycle'], j))\n",
    "            weights[j].append(getWeights(values[i]['result'], ensembleMode))\n",
    "            print (\"Model\", i, \"is\", values[i]['cycle'], \"with\", ensembleMode, \"=\", values[i]['result'])\n",
    "    weights[j] = np.array(weights[j])\n",
    "    weights[j] = weights[j]/weights[j].sum() #normalise weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response of ensemble on development data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev0 = inputPipe[0].transform(devData[regTrainFeatures0].values.astype(theano.config.floatX))\n",
    "X_dev1 = inputPipe[0].transform(devData[regTrainFeatures1].values.astype(theano.config.floatX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred0 = np.zeros((len(devData), len(regTargetFeatures0)))\n",
    "for i, model in enumerate(ensemble[0]):\n",
    "    pred0 += weights[0][i]*outputPipe[0].inverse_transform(model.predict(X_dev0, verbose=0)) \n",
    "pred1 = np.zeros((len(devData), len(regTargetFeatures1)))\n",
    "for i, model in enumerate(ensemble[1]):\n",
    "    pred1 += weights[1][i]*outputPipe[1].inverse_transform(model.predict(X_dev1, verbose=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n, feature in enumerate(regTargetFeatures0):\n",
    "    devData['reg_' + feature[4:]] = pandas.Series(pred0[:,n], index=devData.index)\n",
    "for n, feature in enumerate(regTargetFeatures1):\n",
    "    devData['reg_' + feature[4:]] = pandas.Series(pred1[:,n], index=devData.index)\n",
    "bMass = 4.8\n",
    "devData['reg_b_0_E'] = np.sqrt((bMass**2)+np.square(devData.loc[:,'reg_b_0_px'])+np.square(devData.loc[:,'reg_b_0_py'])+np.square(devData.loc[:,'reg_b_0_pz']))\n",
    "devData['reg_b_1_E'] = np.sqrt((bMass**2)+np.square(devData.loc[:,'reg_b_1_px'])+np.square(devData.loc[:,'reg_b_1_py'])+np.square(devData.loc[:,'reg_b_1_pz']))\n",
    "devData['reg_h_bb_px'] = devData.loc[:,'reg_b_0_px']+devData.loc[:,'reg_b_1_px']\n",
    "devData['reg_h_bb_py'] = devData.loc[:,'reg_b_0_py']+devData.loc[:,'reg_b_1_py']\n",
    "devData['reg_h_bb_pz'] = devData.loc[:,'reg_b_0_pz']+devData.loc[:,'reg_b_1_pz']\n",
    "devData['reg_h_bb_E'] = devData.loc[:,'reg_b_0_E']+devData.loc[:,'reg_b_1_E']\n",
    "devData['reg_h_bb_p2'] = np.square(devData.loc[:,'reg_h_bb_px'])+np.square(devData.loc[:,'reg_h_bb_py'])+np.square(devData.loc[:,'reg_h_bb_pz'])\n",
    "devData['reg_h_bb_mass'] = np.sqrt(np.square(devData.loc[:,'reg_h_bb_E'])-devData.loc[:,'reg_h_bb_p2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devData['loss_d'] = 0\n",
    "for n, feature in enumerate(regTargetFeatures):\n",
    "    devData.loc[:, 'loss_d'] += np.square(devData.loc[:, 'reg_' + feature[4:]]-devData.loc[:, feature])\n",
    "meanBootReg = []\n",
    "stdevBootReg = []\n",
    "for i in range(100):\n",
    "    points = np.random.choice(devData[:]['reg_h_bb_mass'].values, len(devData), replace=True)\n",
    "    meanBootReg.append(points.mean())\n",
    "    stdevBootReg.append(points.std())\n",
    "meanReg = (np.mean(meanBootReg), np.std(meanBootReg)/math.sqrt(len(meanBootReg)))\n",
    "stdevReg = (np.mean(stdevBootReg), np.std(stdevBootReg)/math.sqrt(len(stdevBootReg)))\n",
    "print ('Ensemble on dev data:\\n Loss = {}, Mean = {} +- {}, sigma = {} +- {}'.format(np.mean(devData.loc[:, 'loss_d'])/len(regTargetFeatures), meanReg[0], meanReg[1], stdevReg[0], stdevReg[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response of ensemble on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val0 = inputPipe[0].transform(valData[regTrainFeatures0].values.astype(theano.config.floatX))\n",
    "X_val1 = inputPipe[1].transform(valData[regTrainFeatures1].values.astype(theano.config.floatX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred0 = np.zeros((len(valData), len(regTargetFeatures0)))\n",
    "for i, model in enumerate(ensemble[0]):\n",
    "    pred0 += weights[0][i]*outputPipe[0].inverse_transform(model.predict(X_val0, verbose=0)) \n",
    "pred1 = np.zeros((len(valData), len(regTargetFeatures1)))\n",
    "for i, model in enumerate(ensemble[1]):\n",
    "    pred1 += weights[1][i]*outputPipe[1].inverse_transform(model.predict(X_val1, verbose=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n, feature in enumerate(regTargetFeatures0):\n",
    "    valData['reg_' + feature[4:]] = pandas.Series(pred0[:,n], index=valData.index)\n",
    "for n, feature in enumerate(regTargetFeatures1):\n",
    "    valData['reg_' + feature[4:]] = pandas.Series(pred1[:,n], index=valData.index)\n",
    "bMass = 4.8\n",
    "valData['reg_b_0_E'] = np.sqrt((bMass**2)+np.square(valData.loc[:,'reg_b_0_px'])+np.square(valData.loc[:,'reg_b_0_py'])+np.square(valData.loc[:,'reg_b_0_pz']))\n",
    "valData['reg_b_1_E'] = np.sqrt((bMass**2)+np.square(valData.loc[:,'reg_b_1_px'])+np.square(valData.loc[:,'reg_b_1_py'])+np.square(valData.loc[:,'reg_b_1_pz']))\n",
    "valData['reg_h_bb_px'] = valData.loc[:,'reg_b_0_px']+valData.loc[:,'reg_b_1_px']\n",
    "valData['reg_h_bb_py'] = valData.loc[:,'reg_b_0_py']+valData.loc[:,'reg_b_1_py']\n",
    "valData['reg_h_bb_pz'] = valData.loc[:,'reg_b_0_pz']+valData.loc[:,'reg_b_1_pz']\n",
    "valData['reg_h_bb_E'] = valData.loc[:,'reg_b_0_E']+valData.loc[:,'reg_b_1_E']\n",
    "valData['reg_h_bb_p2'] = np.square(valData.loc[:,'reg_h_bb_px'])+np.square(valData.loc[:,'reg_h_bb_py'])+np.square(valData.loc[:,'reg_h_bb_pz'])\n",
    "valData['reg_h_bb_mass'] = np.sqrt(np.square(valData.loc[:,'reg_h_bb_E'])-valData.loc[:,'reg_h_bb_p2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valData['loss_v'] = 0\n",
    "for n, feature in enumerate(regTargetFeatures):\n",
    "    valData.loc[sig, 'loss_v'] += np.square(valData.loc[sig, 'reg_' + feature[4:]]-valData.loc[sig, feature])\n",
    "meanBootReg = []\n",
    "stvalBootReg = []\n",
    "for i in range(100):\n",
    "    points = np.random.choice(valData[sig]['reg_h_bb_mass'].values, len(valData[sig]), replace=True)\n",
    "    meanBootReg.append(points.mean())\n",
    "    stvalBootReg.append(points.std())\n",
    "meanReg = (np.mean(meanBootReg), np.std(meanBootReg)/math.sqrt(len(meanBootReg)))\n",
    "stvalReg = (np.mean(stvalBootReg), np.std(stvalBootReg)/math.sqrt(len(stvalBootReg)))\n",
    "print ('Ensemble on val data:\\n Loss = {}, Mean = {} +- {}, sigma = {} +- {}'.format(np.mean(valData.loc[sig, 'loss_v'])/len(regTargetFeatures), meanReg[0], meanReg[1], stvalReg[0], stvalReg[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"for n, feature in enumerate(regTargetFeatures):\n",
    "    print ()\n",
    "    print (feature)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "  \n",
    "    bootGen = []\n",
    "    meanBootGen = []\n",
    "    stdevBootGen = []\n",
    "    for i in range(100):\n",
    "        points = np.random.choice(valData[sig][feature].values, len(valData[sig]), replace=True)\n",
    "        meanBootGen.append(points.mean())\n",
    "        stdevBootGen.append(points.std())\n",
    "    meanGen = (np.mean(meanBootGen), np.std(meanBootGen)/math.sqrt(len(meanBootGen)))\n",
    "    stdevGen = (np.mean(stdevBootGen), np.std(stdevBootGen)/math.sqrt(len(stdevBootGen)))\n",
    "    sns.kdeplot(valData[sig][feature])\n",
    "    bootRec = []\n",
    "    meanBootRec = []\n",
    "    stdevBootRec = []\n",
    "    for i in range(100):\n",
    "        points = np.random.choice(valData[sig][feature[4:]].values, len(valData[sig]), replace=True)\n",
    "        meanBootRec.append(points.mean())\n",
    "        stdevBootRec.append(points.std())\n",
    "    meanRec = (np.mean(meanBootRec), np.std(meanBootRec)/math.sqrt(len(meanBootRec)))\n",
    "    stdevRec = (np.mean(stdevBootRec), np.std(stdevBootRec)/math.sqrt(len(stdevBootRec)))\n",
    "    sns.kdeplot(valData[sig][feature[4:]])\n",
    "    bootReg = []\n",
    "    meanBootReg = []\n",
    "    stdevBootReg = []\n",
    "    for i in range(100):\n",
    "        points = np.random.choice(valData[sig][\"reg_\" + feature[4:]].values, len(valData[sig]), replace=True)\n",
    "        meanBootReg.append(points.mean())\n",
    "        stdevBootReg.append(points.std())\n",
    "    meanReg = (np.mean(meanBootReg), np.std(meanBootReg)/math.sqrt(len(meanBootReg)))\n",
    "    stdevReg = (np.mean(stdevBootReg), np.std(stdevBootReg)/math.sqrt(len(stdevBootReg)))\n",
    "    sns.kdeplot(valData[sig][\"reg_\" + feature[4:]])\n",
    "    plt.legend(fontsize=16)\n",
    "    var = \"p_{x,b_0}\"\n",
    "    if \"py\" in feature:\n",
    "        var = \"p_{y,b_0}\"\n",
    "    if \"pz\" in feature:\n",
    "        var = \"p_{z,b_0}\"\n",
    "    if \"_1_\" in feature:\n",
    "        var = var[:-2] + \"1}\"\n",
    "    plt.xlabel(r\"$\" + var + r\"\\ [GeV]$\", fontsize=24, color='black')\n",
    "    plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{d\" + var + r\"}\\ [GeV^{-1}]$\", fontsize=24, color='black')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for n, feature in enumerate(regTargetFeatures):\n",
    "    print ()\n",
    "    print (feature)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    boot = []\n",
    "    meanBoot = []\n",
    "    stdevBoot = []\n",
    "    for i in range(100):\n",
    "        points = np.random.choice(valData[bkg][feature[4:]].values, len(valData[bkg]), replace=True)\n",
    "        meanBoot.append(points.mean())\n",
    "        stdevBoot.append(points.std())\n",
    "    mean = (np.mean(meanBoot), np.std(meanBoot)/math.sqrt(len(meanBoot)))\n",
    "    stdev = (np.mean(stdevBoot), np.std(stdevBoot)/math.sqrt(len(stdevBoot)))\n",
    "    sns.kdeplot(data=valData[bkg][feature[4:]].values, label='bkg reco')\n",
    "    boot = []\n",
    "    meanBoot = []\n",
    "    stdevBoot = []\n",
    "    for i in range(100):\n",
    "        points = np.random.choice(valData[bkg][\"reg_\" + feature[4:]].values, len(valData[bkg]), replace=True)\n",
    "        meanBoot.append(points.mean())\n",
    "        stdevBoot.append(points.std())\n",
    "    mean = (np.mean(meanBoot), np.std(meanBoot)/math.sqrt(len(meanBoot)))\n",
    "    stdev = (np.mean(stdevBoot), np.std(stdevBoot)/math.sqrt(len(stdevBoot)))\n",
    "    sns.kdeplot(data=valData[bkg][\"reg_\" + feature[4:]].values, label='bkg reg')\n",
    "    boot = []\n",
    "    meanBoot = []\n",
    "    stdevBoot = []\n",
    "    for i in range(100):\n",
    "        points = np.random.choice(valData[sig][feature[4:]].values, len(valData[sig]), replace=True)\n",
    "        meanBoot.append(points.mean())\n",
    "        stdevBoot.append(points.std())\n",
    "    mean = (np.mean(meanBoot), np.std(meanBoot)/math.sqrt(len(meanBoot)))\n",
    "    stdev = (np.mean(stdevBoot), np.std(stdevBoot)/math.sqrt(len(stdevBoot)))\n",
    "    sns.kdeplot(data=valData[sig][feature[4:]].values, label='sig recon')\n",
    "    boot = [];\n",
    "    meanBoot = []\n",
    "    stdevBoot = []\n",
    "    for i in range(100):\n",
    "        points = np.random.choice(valData[sig][\"reg_\" + feature[4:]].values, len(valData[sig]), replace=True)\n",
    "        meanBoot.append(points.mean())\n",
    "        stdevBoot.append(points.std())\n",
    "    mean = (np.mean(meanBoot), np.std(meanBoot)/math.sqrt(len(meanBoot)))\n",
    "    stdev = (np.mean(stdevBoot), np.std(stdevBoot)/math.sqrt(len(stdevBoot)))\n",
    "    sns.kdeplot(data=valData[sig][\"reg_\" + feature[4:]].values, label='sig reg')\n",
    "    plt.legend(fontsize=16)\n",
    "    var = \"p_{x,b_0}\"\n",
    "    if \"py\" in feature:\n",
    "        var = \"p_{y,b_0}\"\n",
    "    if \"pz\" in feature:\n",
    "        var = \"p_{z,b_0}\"\n",
    "    if \"_1_\" in feature:\n",
    "        var = var[:-2] + \"1}\"\n",
    "    plt.xlabel(r\"$\" + var + r\"\\ [GeV]$\", fontsize=24, color='black')\n",
    "    plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{d\" + var + r\"}\\ [GeV^{-1}]$\", fontsize=24, color='black')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum pull distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"print (\"\\tFeature\\tmean\\t\\t\\t\\t\\t\\tsigma\")\n",
    "for n, feature in enumerate(regTargetFeatures):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    var = \"p_{x,b_0\"\n",
    "    if \"py\" in feature:\n",
    "        var = \"p_{y,b_0\"\n",
    "    if \"pz\" in feature:\n",
    "        var = \"p_{z,b_0\"\n",
    "    if \"_1_\" in feature:\n",
    "        var = var[:-1] + \"1\"\n",
    "    boot = []\n",
    "    meanBoot = []\n",
    "    stdevBoot = []\n",
    "    pullVal = valData[sig][feature[4:]].values-valData[sig][feature].values\n",
    "    for i in range(100):\n",
    "        points = np.random.choice(pullVal, len(pullVal), replace=True)\n",
    "        meanBoot.append(points.mean())\n",
    "        stdevBoot.append(points.std())\n",
    "    mean = (np.mean(meanBoot), np.std(meanBoot)/math.sqrt(len(meanBoot)))\n",
    "    stdev = (np.mean(stdevBoot), np.std(stdevBoot)/math.sqrt(len(stdevBoot)))\n",
    "    print (\"Reco:\\t{}\\t{} +- {}\\t{} +- {}\".format(feature[4:], mean[0], mean[1], stdev[0], stdev[1]))\n",
    "    sns.kdeplot(data=pullVal, label='reconstructed', gridsize=500)\n",
    "    boot = []\n",
    "    meanBoot = []\n",
    "    stdevBoot = []\n",
    "    pullVal = valData[sig]['reg_' + feature[4:]].values-valData[sig][feature].values\n",
    "    for i in range(100):\n",
    "        points = np.random.choice(pullVal, len(pullVal), replace=True)\n",
    "        meanBoot.append(points.mean())\n",
    "        stdevBoot.append(points.std())\n",
    "       \n",
    "    mean = (np.mean(meanBoot), np.std(meanBoot)/math.sqrt(len(meanBoot)))\n",
    "    stdev = (np.mean(stdevBoot), np.std(stdevBoot)/math.sqrt(len(stdevBoot)))\n",
    "    print (\"Reg:\\t{}\\t{} +- {}\\t{} +- {}\".format(feature[4:], mean[0], mean[1], stdev[0], stdev[1]))\n",
    "    sns.kdeplot(data=pullVal, label='regressed', gridsize=500)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.xlabel(r\"$\" + var + r\",\\mathrm{Est.}}-\" + var + r\",\\mathrm{True}}\\ [GeV]$\", fontsize=24, color='black')\n",
    "    plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{d\\Delta \" + var + r\"}}\\ [GeV^{-1}]$\", fontsize=24, color='black')\n",
    "    plt.xlim(-100,100)\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-sample Kolmogorovâ€“Smirnov test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, feature in enumerate(regTargetFeatures):\n",
    "    print ()\n",
    "    print (feature)\n",
    "    ksTest = ks_2samp(valData[sig][feature].values, valData[sig]['reg_' + feature[4:]].values)\n",
    "    print (\"K-S test result {0:.4f}, p-value of {1:.4f}\".format(ksTest[0], ksTest[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higgs mass distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bootReg = []\n",
    "meanBootReg = []\n",
    "stdevBootReg = []\n",
    "for i in range(100):\n",
    "    points = np.random.choice(valData[sig]['reg_h_bb_mass'].values, len(valData[sig]), replace=True)\n",
    "    meanBootReg.append(points.mean())\n",
    "    stdevBootReg.append(points.std())\n",
    "meanReg = (np.mean(meanBootReg), np.std(meanBootReg)/math.sqrt(len(meanBootReg)))\n",
    "stdevReg = (np.mean(stdevBootReg), np.std(stdevBootReg)/math.sqrt(len(stdevBootReg)))\n",
    "bootReco = []\n",
    "meanBootReco = []\n",
    "stdevBootReco = []\n",
    "for i in range(100):\n",
    "    points = np.random.choice(valData[sig]['h_bb_mass'].values, len(valData[sig]), replace=True)\n",
    "    meanBootReco.append(points.mean())\n",
    "    stdevBootReco.append(points.std())\n",
    "    \n",
    "meanReco = (np.mean(meanBootReco), np.std(meanBootReco)/math.sqrt(len(meanBootReco)))\n",
    "stdevReco = (np.mean(stdevBootReco), np.std(stdevBootReco)/math.sqrt(len(stdevBootReco)))\n",
    "print (\"Distribution\\t\\tmean\\tsigma\")\n",
    "print ('Regressed Signal, Mean = {} +- {}, sigma = {} +- {}'.format(meanReg[0], meanReg[1], stdevReg[0], stdevReg[1]))\n",
    "print ('Reconstructed Signal,  Mean = {:.2f} +- {:.2f}, sigma = {:.2f} +- {:.2f}'.format(meanReco[0], meanReco[1], stdevReco[0], stdevReco[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.kdeplot(data=valData[sig]['reg_h_bb_mass'].values, label='signal regressed')\n",
    "sns.kdeplot(data=valData[sig]['h_bb_mass'].values, label='signal reconstructed')\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(r'$M_{h\\rightarrow b\\bar{b}}\\ [GeV]$' , fontsize=24, color='black')\n",
    "plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{d M}\\ [GeV^{-1}]$\", fontsize=24, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootReg = []\n",
    "meanBootReg = []\n",
    "stdevBootReg = []\n",
    "for i in range(100):\n",
    "    points = np.random.choice((valData[sig]['reg_h_bb_mass'].values-125)/125, len(valData[sig]), replace=True)\n",
    "    meanBootReg.append(points.mean())\n",
    "    stdevBootReg.append(points.std())\n",
    "meanReg = (np.mean(meanBootReg), np.std(meanBootReg)/math.sqrt(len(meanBootReg)))\n",
    "stdevReg = (np.mean(stdevBootReg), np.std(stdevBootReg)/math.sqrt(len(stdevBootReg)))\n",
    "bootReco = []\n",
    "meanBootReco = []\n",
    "stdevBootReco = []\n",
    "for i in range(100):\n",
    "    points = np.random.choice((valData[sig]['h_bb_mass'].values-125)/125, len(valData[sig]), replace=True)\n",
    "    meanBootReco.append(points.mean())\n",
    "    stdevBootReco.append(points.std())\n",
    "meanReco = (np.mean(meanBootReco), np.std(meanBootReco)/math.sqrt(len(meanBootReco)))\n",
    "stdevReco = (np.mean(stdevBootReco), np.std(stdevBootReco)/math.sqrt(len(stdevBootReco)))\n",
    "print (\"Distribution\\t\\tmean\\tsigma\")\n",
    "print ('Regressed Signal, Mean = {} +- {}, sigma = {} +- {}'.format(meanReg[0], meanReg[1], stdevReg[0], stdevReg[1]))\n",
    "print ('Reconstructed Signal,  Mean = {} +- {}, sigma = {} +- {}'.format(meanReco[0], meanReco[1], stdevReco[0], stdevReco[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_params = {'shade' : False}\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.kdeplot(data=(valData[sig]['reg_h_bb_mass'].values-125)/125, label='signal regressed')\n",
    "sns.kdeplot(data=(valData[sig]['h_bb_mass'].values-125)/125, label='signal reconstructed')\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(r\"$\\frac{M_{h,\\mathrm{Est.}}-M_{h,\\mathrm{True}}}{M_{h,\\mathrm{True}}}$\", fontsize=24, color='black')\n",
    "plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{d\\frac{\\Delta M}{M}}$\", fontsize=24, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.kdeplot(data=valData[sig]['reg_h_bb_mass'].values, label = 'signal')\n",
    "sns.kdeplot(data=valData[sigMM]['reg_h_bb_mass'].values, label = 'signal (match)')\n",
    "sns.kdeplot(data=valData[bkg]['reg_h_bb_mass'].values, label = 'background')\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.xlabel(r'$M_{h\\rightarrow b\\bar{b}}\\ [GeV]$' , fontsize=24, color='black')\n",
    "plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{d M}\\ [GeV^{-1}]$\", fontsize=24, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xPlot = np.linspace(0, 300, 300)[:, np.newaxis]\n",
    "plots = mpRun([{'data':valData.ix[bkg, 'h_bb_mass'], 'x':xPlot, 'name':'bkg', 'kde':1, 'mean':1, 'std':1},\n",
    "               {'data':valData.ix[bkg, 'reg_h_bb_mass'], 'x':xPlot, 'name':'bkg_reg', 'kde':1, 'mean':1, 'std':1},\n",
    "               {'data':valData.ix[sig, 'h_bb_mass'], 'x':xPlot, 'name':'sig', 'kde':1, 'mean':1, 'std':1},\n",
    "               {'data':valData.ix[sig, 'reg_h_bb_mass'], 'x':xPlot, 'name':'sig_reg', 'kde':1, 'mean':1, 'std':1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meanReco = (np.mean(plots['sig' + '_mean']),\n",
    "            np.std(plots['sig' + '_mean'])/math.sqrt(len(plots['sig' + '_mean'])))\n",
    "stdReco = (np.mean(plots['sig' + '_std']),\n",
    "             np.std(plots['sig' + '_std'])/math.sqrt(len(plots['sig' + '_std'])))\n",
    "meanReg = (np.mean(plots['sig_reg' + '_mean']),\n",
    "            np.std(plots['sig_reg' + '_mean'])/math.sqrt(len(plots['sig_reg' + '_mean'])))\n",
    "stdReg = (np.mean(plots['sig_reg' + '_std']),\n",
    "             np.std(plots['sig_reg' + '_std'])/math.sqrt(len(plots['sig_reg' + '_std'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.kdeplot(data=valData.loc[sig, 'reg_h_bb_mass'].values, label='signal regressed', color = 'blue')\n",
    "sns.kdeplot(data=valData.loc[sig, 'h_bb_mass'].values, label='signal reconstructed', color = 'blue' , ls = 'dashed')\n",
    "\n",
    "sns.kdeplot(data=valData.loc[bkg, 'reg_h_bb_mass'].values, label='background regressed', color = 'green' )\n",
    "sns.kdeplot(data=valData.loc[bkg, 'h_bb_mass'].values, label='background reconstructed', color = 'green', ls = 'dashed')\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlim(0,300)\n",
    "plt.xlabel(r'$M_{h\\rightarrow b\\bar{b}}\\ [GeV]$' , fontsize=24, color='black')\n",
    "plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{d M}\\ [GeV^{-1}]$\", fontsize=24, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Reg:', roc_auc_score(valData.ix[sig|bkg, 'gen_target'], -np.abs(125-valData.ix[sig|bkg, 'reg_h_bb_mass'])))\n",
    "print ('Reco:', roc_auc_score(valData.ix[sig|bkg, 'gen_target'], -np.abs(125-valData.ix[sig|bkg, 'h_bb_mass'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(-np.abs(125-valData.loc[sig, 'reg_h_bb_mass']), label='signal')\n",
    "sns.distplot(-np.abs(125-valData.loc[bkg, 'reg_h_bb_mass']), label='bkg')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classFeatures = ['b_0_mass', 'b_1_mass', 't_0_mass', 't_1_mass', 't_0_px', 't_0_py', 't_0_pz', 't_1_px', 't_1_py', 't_1_pz', 'b_0_px', 'b_0_py', 'b_0_pz', 'b_1_px', 'b_1_py', 'b_1_pz', 'mPT_px', 'mPT_py', 'b_0_|p|', 'b_0_E', 'b_1_|p|', 'b_1_E', 't_0_|p|', 't_0_E', 't_1_|p|', 't_1_E', 'diH_mass', 'h_tt_mass', 'h_tt_px', 'h_tt_py', 'h_tt_pz', 'h_bb_px', 'h_bb_py', 'h_bb_pz', 'diH_px', 'diH_py', 'diH_pz', 'diH_|p|', 'diH_E', 'h_bb_|p|', 'h_bb_E', 'h_tt_|p|', 'h_tt_E', 'hl_mT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoFeatures = ['h_bb_mass']#, 'b_0_px', 'b_0_py', 'b_0_pz', 'b_1_px', 'b_1_py', 'b_1_pz']#, 'h_tt_mass', 'diH_mass'] #classFeatures + ['h_bb_mass']\n",
    "#recoFeatures = ['b_0_px', 'b_0_py', 'b_0_pz', 'b_1_px', 'b_1_py', 'b_1_pz']#, 'h_tt_mass', 'diH_mass'] #classFeatures + ['h_bb_mass']\n",
    "recoFeatures = classFeatures + ['h_bb_mass']\n",
    "recoClass = xgb.XGBClassifier(base_score=0.5, learning_rate=0.3,\n",
    "         gamma=0, max_depth=6, missing=-999.0, n_estimators=100, random_state=0,n_jobs=8)\n",
    "recoClass.fit(valData[recoFeatures], valData['gen_target'].values.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regFeatures = ['reg_h_bb_mass']#, 'reg_b_0_px', 'reg_b_0_py', 'reg_b_0_pz', 'reg_b_1_px', 'reg_b_1_py', 'reg_b_1_pz']#, 'h_tt_mass', 'diH_mass'] #classFeatures + ['reg_h_bb_mass']\n",
    "#regFeatures = ['reg_b_0_px', 'reg_b_0_py', 'reg_b_0_pz', 'reg_b_1_px', 'reg_b_1_py', 'reg_b_1_pz']#, 'h_tt_mass', 'diH_mass'] #classFeatures + ['reg_h_bb_mass']\n",
    "regFeatures = classFeatures + ['reg_h_bb_mass']\n",
    "regClass = xgb.XGBClassifier(base_score=0.5, learning_rate=0.3,\n",
    "         gamma=0, max_depth=6, missing=-999.0, n_estimators=100, random_state=0,n_jobs=8)\n",
    "regClass.fit(valData[regFeatures], valData['gen_target'].values.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recoPred = recoClass.predict_proba(valData[recoFeatures])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regPred = regClass.predict_proba(valData[regFeatures])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recoAUC = roc_auc_score(valData['gen_target'].values, recoPred)\n",
    "regAUC = roc_auc_score(valData['gen_target'].values, regPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 8])\n",
    "plt.plot(*roc_curve(valData['gen_target'].values, recoPred)[:2],\n",
    "         label=r'Reco Val, $auc={:.4f}$'.format(recoAUC), color='g')\n",
    "plt.plot(*roc_curve(valData['gen_target'].values, regPred)[:2],\n",
    "         label=r'Reg Val, $auc={:.4f}$'.format(regAUC), color='b')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='No discrimination')\n",
    "plt.xlabel('Background acceptance', fontsize=24, color='black')\n",
    "plt.ylabel('Signal acceptance', fontsize=24, color='black')\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name = \"weights/NN_B_Regressor_SO_App_\" + mode + \"_\"\n",
    "print (name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.system(\"rm \" + name + \"*.json\")\n",
    "os.system(\"rm \" + name + \"*.h5\")\n",
    "os.system(\"rm \" + name + \"*.pkl\")\n",
    "for j in range(0,2):\n",
    "    for i, model in enumerate(ensemble[j]):\n",
    "        json_string = model.to_json()\n",
    "        open(name + '_' + str(i) + \"_\" + str(j) +'.json', 'w').write(json_string)\n",
    "        model.save_weights(name + '_' + str(i) + \"_\" + str(j) + '.h5')\n",
    "    with open(name  + str(j) + '_compile.json', 'w') as fout:\n",
    "        json.dump(compileArgs[j], fout)\n",
    "    with open(name  + str(j) +'_weights.pkl', 'w') as fout:\n",
    "        pickle.dump(weights[j], fout)\n",
    "    with open(name  + str(j) +'_inputPipe.pkl', 'w') as fout:\n",
    "        pickle.dump(inputPipe[j], fout)\n",
    "    with open(name  + str(j) + '_outputPipe.pkl', 'w') as fout:\n",
    "        pickle.dump(outputPipe[j], fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble = [[],[]]\n",
    "weights = [None,None]\n",
    "inputPipe = [None,None]\n",
    "outputPipe = [None,None]\n",
    "compileArgs = [None,None]\n",
    "for j in range(0,2):\n",
    "    with open(name + str (j) +'_compile.json', 'r') as fin:\n",
    "        compileArgs[j] = json.load(fin)\n",
    "    for i in range(ensembleSize):\n",
    "        model = model_from_json(open(name + '_' + str(i) + '_' + str (j) + '.json').read())\n",
    "        model.load_weights(name + \"_\" + str(i) + \"_\" + str (j) + '.h5')\n",
    "        model.compile(**compileArgs[j])\n",
    "        ensemble[j].append(model)\n",
    "    with open(name + str (j)+ '_weights.pkl', 'r') as fin:\n",
    "        weights[j] = pickle.load(fin)\n",
    "    with open(name + str (j)+ '_inputPipe.pkl', 'r') as fin:\n",
    "        inputPipe[j] = pickle.load(fin)\n",
    "    with open(name + str (j)+ '_outputPipe.pkl', 'r') as fin:\n",
    "        outputPipe[j] = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
