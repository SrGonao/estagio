{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN ensemble classifier\n",
    "Separate signal ($gg\\rightarrow hh\\rightarrow b\\bar{b}\\tau\\bar{\\tau}$) from background ($pp\\rightarrow t\\bar{t}\\rightarrow \\ell\\bar{\\ell}b\\bar{b},\\ \\ell\\in\\{e,\\mu,\\tau\\}$) using an ensemble of neural networks. Focus is on $\\mu\\tau_{h}bb$ channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas\n",
    "import timeit #more accurate than time\n",
    "\n",
    "import theano\n",
    "import tensorflow\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, TensorBoard,ReduceLROnPlateau\n",
    "from keras.layers import Dense, Activation,Dropout\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "from keras.constraints import maxnorm\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers.noise import GaussianNoise\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "import os\n",
    "import json\n",
    "\n",
    "from six.moves import cPickle #Faster than pickle\n",
    "import sys\n",
    "sys.path.append('../../modules/')\n",
    "from MPPlot import *\n",
    "from Processors import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "    \"\"\"\n",
    "    Changes Keras backend\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    backend : str\n",
    "              Backend wanted (theano or tensorflow)\n",
    "              \n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    \n",
    "    idm = {\"theano\": \"th\", \"tensorflow\": \"tf\"}\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        importlib.reload(K)\n",
    "        assert K.backend() == backend\n",
    "        keras.backend.set_image_dim_ordering(idm[backend])\n",
    "\n",
    "set_keras_backend(\"theano\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Read ROOT files into Pandas dataframe. Pandas is a very fast and flexibly way of handling data in Python. It stores data in RAM, so doesn't handle large files well...\n",
    "\n",
    "Data is available in both ROOT and CSV format. Pick which you wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useROOT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples contains 10417 signal events and 168053 background events\n",
      "178470 events in total\n"
     ]
    }
   ],
   "source": [
    "loc = \"../Data/\"\n",
    "SignalData = []\n",
    "BackgroundData = []\n",
    "\n",
    "masses = [260, 270, 280, 300, 320, 400, 500, 550, 600 ,700]\n",
    "backgrounds = ['TT' , 'WJetsToLNu']\n",
    "Data = np.concatenate((masses,backgrounds))\n",
    "\n",
    "if useROOT: #Load data from ROOT files\n",
    "    import root_numpy\n",
    "    signalData = pandas.DataFrame(root_numpy.root2array(loc + \"signal.root\", treename = 'tree'))\n",
    "    backgroundData = pandas.DataFrame(root_numpy.root2array(loc + \"background.root\", treename = 'tree'))\n",
    "\n",
    "else: #Load data from CSV files\n",
    "    for mass in masses:\n",
    "        signaldata = pandas.read_csv(loc + \"GluGluToRadionToHHTo2B2Tau_M-\" + str(mass) +\"_narrow_13TeV-madgraph.csv\")\n",
    "        signaldata['gen_label'] = str(mass)\n",
    "        SignalData.append(signaldata)\n",
    "        \n",
    "        \n",
    "    names = { 'TT':loc + \"TT_TuneCUETP8M1_13TeV-powheg-pythia8_2.csv\",\n",
    "              'WJetsToLNu':loc + \"WJetsToLNu_TuneCUETP8M1_13TeV-amcatnloFXFX-pythia8.csv\"}\n",
    "    for back in backgrounds:\n",
    "        backgroundData = pandas.read_csv(names[back])\n",
    "        backgroundData['gen_label'] = back\n",
    "        BackgroundData.append(backgroundData)    \n",
    "     \n",
    "    \n",
    "signalData = SignalData[0]\n",
    "for signal_data in SignalData[1:]:\n",
    "    signalData = signalData.append(signal_data, ignore_index=True)    \n",
    "signalData.drop([x for x in signalData.columns if 'Unnamed' in x], axis=1, inplace=True)    \n",
    "signalData['gen_target'] = pandas.Series(np.ones(signalData.size))\n",
    "backgroundData = BackgroundData[0]\n",
    "for background_data in BackgroundData[1:]:\n",
    "    backgroundData = backgroundData.append(background_data, ignore_index=True)    \n",
    "backgroundData.drop([x for x in backgroundData.columns if 'Unnamed' in x], axis=1, inplace=True)    \n",
    "backgroundData['gen_target'] = pandas.Series(np.zeros(backgroundData.size))\n",
    "\"\"\"else: #Load data from CSV files\n",
    "    signalData = pandas.read_csv(loc + \"signal.csv\")\n",
    "    backgroundData = pandas.read_csv(loc + \"background.csv\")\n",
    "    \n",
    "    # Drop columns without header\n",
    "    signalData.drop([x for x in signalData.columns if 'Unnamed' in x], axis=1, inplace=True)\n",
    "    backgroundData.drop([x for x in backgroundData.columns if 'Unnamed' in x], axis=1, inplace=True)  \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"Samples contains {0} signal events and {1} background events\".format(len(signalData), len(backgroundData)))\n",
    "print(\"{} events in total\".format(len(signalData)+len(backgroundData)))\n",
    "data = signalData.append(backgroundData, ignore_index = True) #Combine into signal dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data and add new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - t_0 = $\\tau_h$\n",
    " - t_1 = $\\mu$\n",
    " - b_0 & b_1 = selected b-jets ordered by $p_T$\n",
    " - mPT = missing transverse-momenta\n",
    " - h_tt = vector-sum of t_0, t_1, and mPT ($h\\rightarrow\\tau\\tau$)\n",
    " - h_bb = vector-sum of b_0 and b_1 ($h\\rightarrow bb$)\n",
    " - diH = vector-sum of h_tt and h_bb (di-Higgs vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrimantion often works better when vectors are in Carteesian coordinates, let's transform our data and add some extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "particles = ['t_0', 't_1', 'b_0', 'b_1', 'h_tt', 'h_bb', 'diH']\n",
    "\n",
    "for p in particles:\n",
    "    moveToCartesian(data, p) #Move pT, eta, and phi to p_x, p_y, and p_z\n",
    "    addEnergy(data, p) #Calculate energy and absolute momentum\n",
    "\n",
    "moveToCartesian(data, 'mPT', False)  #Move Missing pT and phi to p_x and p_y\n",
    "addAbsMom(data, 'mPT', False) #Calculate absolute missing transverse momentum\n",
    "addMT(data, data['t_1_pT'], data['t_1_phi'], 't_1') #Calculate transverse mass of tau_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create development and validation samples\n",
    "Development data is used for training and testing. Validation is used for testing the final classifier. Note that the event selection for signal sometimes selects final-states which don't correspond to the Higgs decay products. gen_mctMatch == 1 indicates that the correct final-sates were chosen. For simplicity we'll just consider these signal events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines lists of indices for signal and background events for the development and validation samples. About 20% of each class is reserved for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sig_devIndeces, sig_valIndeces = \\\n",
    "                train_test_split([i for i in data[(data.gen_target == 1) & (data.gen_mctMatch == 1)].index.tolist()],\n",
    "                                 test_size=0.2, random_state=1337)\n",
    "bkg_devIndeces, bkg_valIndeces = \\\n",
    "                train_test_split([i for i in data[(data.gen_target == 0) & (data.gen_mctMatch == 0)].index.tolist()],\n",
    "                                 test_size=0.2, random_state=1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the full dataset into the dev and val sets, and define aliases for the cuts necessary to select signal and background events in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138371 events for training, 34594 events for validation\n",
      "Dev: 3929 of which are signal and 134442 are background\n",
      "Val: 983 of which are signal and 33611 are background\n"
     ]
    }
   ],
   "source": [
    "devData = data.loc[sig_devIndeces].copy()\n",
    "devData = devData.append(data.loc[bkg_devIndeces].copy(), ignore_index = True)\n",
    "valData = data.loc[sig_valIndeces].copy()\n",
    "valData = valData.append(data.loc[bkg_valIndeces].copy(), ignore_index = True)\n",
    "\n",
    "sigDev = (devData.gen_target == 1) & (devData.gen_mctMatch == 1)\n",
    "bkgDev = (devData.gen_target == 0)\n",
    "sigVal = (valData.gen_target == 1) & (valData.gen_mctMatch == 1)\n",
    "bkgVal = (valData.gen_target == 0)\n",
    "\n",
    "print(\"{} events for training, {} events for validation\".format(len(devData), len(valData)))\n",
    "print(\"Dev: {} of which are signal and {} are background\".format(len(devData[sigDev]), len(devData[bkgDev])))\n",
    "print(\"Val: {} of which are signal and {} are background\".format(len(valData[sigVal]), len(valData[bkgVal])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are more background events, let's define weights to help balance out the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Weights are:  {0.0: 1.0, 1.0: 34.217867141766355}\n"
     ]
    }
   ],
   "source": [
    "classWeights = {0.0 : 1.0,\n",
    "                1.0 : len(devData[bkgDev])/len(devData[sigDev])}\n",
    "\n",
    "print(\"Dev Weights are: \", classWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options\n",
    "Define the features used for discrimination and training options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genFeatures = [var for var in data.columns if str.startswith(var, \"gen\")] #Generator features; not for training\n",
    "trainFeatures = [var for var in data.columns if var not in genFeatures] #Reconstructed features; ok for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer need the old $p_T$, $\\eta$, and $\\phi$ coordinates so remove them from the training features. 't_1_mass' is also constant ($m_{\\mu}$) so let's remove that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_0_mass', 'b_0_mass', 'b_1_mass', 'h_tt_mass', 'h_bb_mass', 'mT', 'hT', 'sT', 'centrality', 'eVis', 'sphericity', 'spherocity', 'aplanarity', 'aplanority', 'upsilon', 'dShape', 'sphericityEigen0', 'sphericityEigen1', 'sphericityEigen2', 'spherocityEigen0', 'spherocityEigen1', 'spherocityEigen2', 't_0_px', 't_0_py', 't_0_pz', 't_0_|p|', 't_0_E', 't_1_px', 't_1_py', 't_1_pz', 't_1_|p|', 't_1_E', 'b_0_px', 'b_0_py', 'b_0_pz', 'b_0_|p|', 'b_0_E', 'b_1_px', 'b_1_py', 'b_1_pz', 'b_1_|p|', 'b_1_E', 'h_tt_px', 'h_tt_py', 'h_tt_pz', 'h_tt_|p|', 'h_tt_E', 'h_bb_px', 'h_bb_py', 'h_bb_pz', 'h_bb_|p|', 'h_bb_E', 'diH_px', 'diH_py', 'diH_pz', 'diH_|p|', 'diH_E', 'mPT_px', 'mPT_py', 'mPT_|p|', 't_1_mT']\n"
     ]
    }
   ],
   "source": [
    "pTEtaPhi = [var for var in trainFeatures for x in ['pT', 'eta', 'phi', 't_1_mass', 'h_tt_svFit_mass', 'diH_kinFit_mass' ,'diH_kinFit_prob' , 'b_0_csv' ,'b_1_csv','diH_mass'] if x in var]\n",
    "trainFeatures = [var for var in trainFeatures if var not in pTEtaPhi]\n",
    "print(trainFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide up the available features into sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsFeatures = [var for var in trainFeatures for p in particles + ['mPT'] if p in var]\n",
    "shapes = [var for var in trainFeatures for x in ['aplan', 'dShape', 'spher', 'upsilon'] if x in var]\n",
    "shapeFeatures = [var for var in trainFeatures if var in shapes]\n",
    "eventKinematicFeatures = ['centrality', 'eVis', 'hT', 'sT']\n",
    "jetFeatures = [var for var in trainFeatures if 'Jet' in var and 'Jets' not in var]\n",
    "multiplicityFeatures = ['nBJets', 'nJets', 'nPhotons', 'nTauJets']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training options\n",
    "Here we define the way we'll train the classifier. For simplicity we'll just use the low-level final-state features. We can also choose what pre-processing step to apply to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 44 features ['t_0_mass', 'b_0_mass', 'b_1_mass', 'h_tt_mass', 'h_bb_mass', 't_0_px', 't_0_py', 't_0_pz', 't_0_|p|', 't_0_E', 't_1_px', 't_1_py', 't_1_pz', 't_1_|p|', 't_1_E', 'b_0_px', 'b_0_py', 'b_0_pz', 'b_0_|p|', 'b_0_E', 'b_1_px', 'b_1_py', 'b_1_pz', 'b_1_|p|', 'b_1_E', 'h_tt_px', 'h_tt_py', 'h_tt_pz', 'h_tt_|p|', 'h_tt_E', 'h_bb_px', 'h_bb_py', 'h_bb_pz', 'h_bb_|p|', 'h_bb_E', 'diH_px', 'diH_py', 'diH_pz', 'diH_|p|', 'diH_E', 'mPT_px', 'mPT_py', 'mPT_|p|', 't_1_mT']\n"
     ]
    }
   ],
   "source": [
    "classTrainFeatures = fsFeatures #The features used\n",
    "classModel = \"model0\" #Will define the layout of the network\n",
    "varSet = \"fsFeatures\" #Name of the feature set used, mainly for saving results\n",
    "normIn = True #Whether we want to normalise and standardise the inputs\n",
    "pca = True #Whether we want to use principal-component analysis to decorrelate inputs\n",
    "whiten = False #Whether we want to whiten input data\n",
    "nSplits = 10 #Number of train/test splits to make during cross-validation\n",
    "ensembleSize = 5 #Number of classifiers  to include in ensemble = min(nSplits, ensembleSize)\n",
    "ensembleMode = 'loss' #Metric used to weight classifiers in ensemble, I've found loss to quite relaible\n",
    "\n",
    "\n",
    "compileArgs = {'loss':'binary_crossentropy', \n",
    "               'optimizer':'adam'} #Loss function and optimiser for NN\n",
    "trainParams = {'epochs' : 10000, \n",
    "               'batch_size' : 64, \n",
    "               'verbose' : 1} #Maximum epochs for training and size of mini-batch\n",
    "print(\"Training on {} features {}\". format(len(classTrainFeatures),[var for var in classTrainFeatures]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a SK-Learn pipeline which will contain transformation steps for any data fed in. Pipelines are a nice, compact way of handing data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepsIn = []\n",
    "if not normIn and not pca:\n",
    "    stepsIn.append(('ident', StandardScaler(with_mean=False, with_std=False))) #For compatability\n",
    "else:\n",
    "    if normIn:\n",
    "        stepsIn.append(('normIn', StandardScaler()))\n",
    "    if pca:\n",
    "        stepsIn.append(('pca', PCA(whiten=whiten)))\n",
    "inputPipe = Pipeline(stepsIn)\n",
    "stepsOut = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit the pipeline to the **development** data inputs. For compactness we also transform the development data and create Numpy arrays of the inputs and targets. **N.B.** The type of the inputs will normally be either float32 or float64. float32 is preferred, since speed and memory outweighs precision. Sometimes if the data is naturally in float64, the conversion to float32 can can result in NaNs or infs, so watch out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['t_0_pT', 't_0_eta', 't_0_phi', 't_0_mass', 't_1_pT', 't_1_eta',\n",
      "       't_1_phi', 't_1_mass', 'b_0_pT', 'b_0_eta',\n",
      "       ...\n",
      "       'h_bb_E', 'diH_px', 'diH_py', 'diH_pz', 'diH_|p|', 'diH_E', 'mPT_px',\n",
      "       'mPT_py', 'mPT_|p|', 't_1_mT'],\n",
      "      dtype='object', length=124)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_class = inputPipe.fit_transform(devData[classTrainFeatures].values.astype('float32')) #Inputs\n",
    "y_class = devData['gen_target'].values.astype('int') #Outputs\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classifier\n",
    "Here we define the layout of the neural network. The basic class is sequential. The network is them defined by adding dense layers (which contain the neurons and weights) and then the activation function. The activation function can be defined directly in the dense constructor, however adding separately gives a clearer picture of the network, and allows it to be easily replaced with an advanced activation-function.\n",
    "\n",
    "For contempory ML, the rectified linear unit is generally the default activation function. Since we want the outputs to be between 0 (background) and 1 (signal) we'll use the sigmoid function as the final activation function, since it saturates at these values and is symmetric between them.\n",
    "\n",
    "The compile step combines the choice of loss function and optimiser into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale*K.elu(x, alpha)\n",
    "\n",
    "\n",
    "\n",
    "def getClassifier(model, nIn, compileArgs):\n",
    "    classModel = Sequential()\n",
    "    depth = None\n",
    "    width = None\n",
    "    if model == \"model0\":\n",
    "        depth = 3\n",
    "        width = 100\n",
    "    classModel.add(Dense(width, input_dim=nIn)) #First layer requires number of inputs\n",
    "    #classModel.add(GaussianNoise(0.01))\n",
    "    classModel.add(Activation('selu'))\n",
    "    \n",
    "    \n",
    "   \n",
    "    for i in range(depth): #Continue to add hidden layers\n",
    "        classModel.add(Dense(width)) #Subsequent layers inherit input_dim from previous layer\n",
    "     \n",
    "        classModel.add(Activation('selu'))\n",
    "       \n",
    "        \n",
    "    classModel.add(Dense(1, activation='sigmoid')) #Final layer requires one output\n",
    "    classModel.compile(**compileArgs) #Compile the network graph to prepare it for use\n",
    "    return classModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks are methods that can be called during training. They have a variety of uses such as monitoring training, stopping training early, and saving different versions of the model. Here we define our own callback,  which saves the history of the training.\n",
    "\n",
    "We want to view the history of the model's performance on the training and testing data during data, however by default the training loss is averaged over the epoch, and the test loss is evaluated at the end of the epoch, so is not comparable. This modified version evaluates the performance on the training data at the end of each epoch.\n",
    "\n",
    "Later well also use some other callbacks:\n",
    "\n",
    "EarlyStopping monitors a specified metric and stops the training if the performance fails to improve for a specified number of epochs in a row. Here we use it to monitor the loss on the test data and stop when it doesn't improve after 10 epochs.\n",
    "\n",
    "ModelCheckpoint is used to save the weights of the network during training. It's quite flexible, but here we use it save the model which performs best according to the loss in test data.\n",
    "\n",
    "Normally during training, the test loss will reach a minimum and either saturate or start to increase (overtraining). The training loss will normally either saturate or continue to decrease. The optimum point is when the test-loss first reaches its minimum point. The combination of EarlyStopping and ModelCheckpoint acts to save the model at this point and allow some leeway in case it starts to decrease again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def __init__(self, trData):\n",
    "        self.trainingData = trData\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {}\n",
    "        self.losses['loss'] = []\n",
    "        self.losses['val_loss'] = []\n",
    "        self.losses['val_acc'] = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses['loss'].append(self.model.evaluate(self.trainingData[0], self.trainingData[1], verbose=0))\n",
    "        self.losses['val_loss'].append(logs.get('val_loss'))\n",
    "        self.losses['val_acc'].append(logs.get('val_acc'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train the classifier.\n",
    "\n",
    "We use stratified k-fold cross-validation for training. This splits the full development data into *k* sets, trains the model on $k-1$ sets and tests its performance on the remainings set. This then continues *k* times with a different set being used for testing each time. The *stratified* part means that each set will contain the same fraction of event classes as the full dataset, which helps ensure unbiased training and means that our class weights will be valid.\n",
    "\n",
    "During training we save each trained model, as well as its performance on the test set.\n",
    "\n",
    "**N.B.** The model can either be saved directly, or by saving the weights and layout separately. The former is more compact, but doesn't handle custom objects well. If you've used a custom loss or activation function, the the second method is more flexible. It also seems to be quicker, but I've not done concrete tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 / 10\n",
      "Train on 124533 samples, validate on 13838 samples\n",
      "Epoch 1/10000\n",
      "124533/124533 [==============================] - 44s - loss: 0.6440 - val_loss: 0.2045\n",
      "Epoch 2/10000\n",
      "124533/124533 [==============================] - 43s - loss: 0.5020 - val_loss: 0.1854\n",
      "Epoch 3/10000\n",
      "124533/124533 [==============================] - 43s - loss: 0.4664 - val_loss: 0.1944\n",
      "Epoch 4/10000\n",
      "124533/124533 [==============================] - 40s - loss: 0.4369 - val_loss: 0.2222\n",
      "Epoch 5/10000\n",
      "124533/124533 [==============================] - 36s - loss: 0.4170 - val_loss: 0.2719\n",
      "Epoch 6/10000\n",
      "124533/124533 [==============================] - 40s - loss: 0.3939 - val_loss: 0.1883\n",
      "Epoch 7/10000\n",
      "124533/124533 [==============================] - 55s - loss: 0.3761 - val_loss: 0.2058\n",
      "Epoch 8/10000\n",
      "124533/124533 [==============================] - 40s - loss: 0.3659 - val_loss: 0.2581\n",
      "Epoch 9/10000\n",
      "124533/124533 [==============================] - 54s - loss: 0.3453 - val_loss: 0.1675\n",
      "Epoch 10/10000\n",
      "124533/124533 [==============================] - 47s - loss: 0.3373 - val_loss: 0.2146\n",
      "Epoch 11/10000\n",
      "102848/124533 [=======================>......] - ETA: 4s - loss: 0.3272"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-81f71f1539ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     model.fit(X_class[train], y_class[train], validation_data = (X_class[test], y_class[test]),\n\u001b[0;32m---> 36\u001b[0;31m               class_weight = classWeights, callbacks = [earlyStop, saveBest, lossHistory], **trainParams)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossHistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Saves the loss history from callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/tensor/blas.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1816\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe78369e898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "results = []\n",
    "histories = []\n",
    "\n",
    "os.system(\"mkdir train_weights\")\n",
    "os.system(\"rm train_weights/*.h5\")\n",
    "os.system(\"rm train_weights/*.json\")\n",
    "os.system(\"rm train_weights/*.pkl\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=nSplits, shuffle=True)\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "i = 0\n",
    "\n",
    "  \n",
    "for train, test in skf.split(X_class, y_class): #test and train are sets of indices for the current CV fold\n",
    "    i += 1\n",
    "    display.clear_output(wait=True)\n",
    "    print(\"Running fold\", i, \"/\", nSplits)\n",
    "    \n",
    "    model = None # Clearing the NN\n",
    "    model = getClassifier(classModel, len(classTrainFeatures), compileArgs)\n",
    "    model.reset_states #Just checking\n",
    "    \n",
    "    lossHistory = LossHistory((X_class[train], y_class[train]))\n",
    "    earlyStop = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=10, verbose=0, mode='auto')\n",
    "    saveBest = ModelCheckpoint(\"train_weights/best.h5\", monitor='val_loss', verbose=0, \n",
    "                              save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "    #tensorboard=TensorBoard(log_dir=\"./logs\", histogram_freq=1, batch_size=128,write_grads=True,embeddings_freq=1)\n",
    "       #Begin training the model\n",
    "    \n",
    "    model.fit(X_class[train], y_class[train], validation_data = (X_class[test], y_class[test]),\n",
    "              class_weight = classWeights, callbacks = [earlyStop, saveBest, lossHistory], **trainParams)\n",
    "    \n",
    "    histories.append(lossHistory.losses) #Saves the loss history from callback  \n",
    "    model.load_weights(\"train_weights/best.h5\") #Loads the best model saved by ModelCheckpoint\n",
    "    plt.plot(histories[i-1]['loss'], color='g',label = 'training')\n",
    "    plt.plot(histories[i-1]['val_loss'], color='b', label = 'testing')\n",
    "    \n",
    "    results.append({})\n",
    "    results[-1]['loss'] = model.evaluate(X_class[test], y_class[test], verbose=1) #Gets loss on test data\n",
    "    results[-1]['AUC'] = 1-roc_auc_score(y_class[test], model.predict(X_class[test], verbose=1)) #Gets ROC AUC for test data\n",
    "    print(\"Score is:\", results[-1])\n",
    "    \n",
    "    model.save('train_weights/train_' + str(i-1)  + '.h5') #Save the model\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.xlabel(\"Epoch\", fontsize=24, color='black')\n",
    "    plt.ylabel(\"Loss\", fontsize=24, color='black')\n",
    "    plt.show()\n",
    "    print(\"\\nCross-validation took {:.3f}s \".format(timeit.default_timer() - start))\n",
    "    \n",
    "with open('train_weights/resultsFile.pkl', 'wb') as fout: #Save results\n",
    "    cPickle.dump(results, fout)\n",
    "\n",
    "\n",
    "\n",
    "X_class = None\n",
    "y_class = None\n",
    "train = None\n",
    "test = None\n",
    "model.summary() #Prints a summary of the model layout\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histories\n",
    "Now let's plot the history of the training.\n",
    "\n",
    "We can see that the test loss starts to decrease, reaches a minimum point, then begins to increase. The training loss continues to decrease. The early stopping detects the lack of imporvement in test loss and stops the training. The checkpoint allows us to use the state of the model at the minimum point of training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    if i == 0:\n",
    "        plt.plot(history['loss'], color='g', label='Training')\n",
    "        plt.plot(history['val_loss'], color='b', label='Testing')\n",
    "    else:\n",
    "        plt.plot(history['loss'], color='g')\n",
    "        plt.plot(history['val_loss'], color='b')\n",
    "        \n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=24, color='black')\n",
    "plt.ylabel(\"Loss\", fontsize=24, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ensemble\n",
    "During the *k*-fold CV we trained *k* models. We could just use the best one, however it is unlikely to optimimum for all input possibilities. Ensembling is a method of using multiple classifiers together to achieve a better result than a single one on its own.\n",
    "\n",
    "The method I use here is to weight the contributions of each classifier according to how well it performed on its test set during CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = None\n",
    "with open('train_weights/resultsFile.pkl', 'rb') as fin: #Reload results in case notebook was closed\n",
    "    results = cPickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadModel(cycle, location='train_weights/train_'): #Function to load a specified classifier\n",
    "    cycle = int(cycle)\n",
    "    model = load_model(location + str(cycle) + '.h5')\n",
    "    model.compile(**compileArgs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWeights(value, met): #How the weight is calculated. Metrics configured such that lower values are better.\n",
    "    return 1/value #Reciprocal of metric is a simple way of assigning larger weight s to better metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we order the classifiers by performance, load the required number, and weight them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "weights = []\n",
    "\n",
    "print(\"Choosing ensemble by\", ensembleMode)\n",
    "dtype = [('cycle', int), ('result', float)]\n",
    "values = np.sort(np.array([(i, result[ensembleMode]) for i, result in enumerate(results)], \n",
    "                          dtype=dtype), order=['result'])\n",
    "\n",
    "for i in range(min([ensembleSize, len(results)])):\n",
    "    ensemble.append(loadModel(values[i]['cycle']))\n",
    "    weights.append(getWeights(values[i]['result'], ensembleMode))\n",
    "    print(\"Model {} is {} with {} = {}\". format(i, values[i]['cycle'], ensembleMode, values[i]['result']))\n",
    "    \n",
    "weights = np.array(weights)\n",
    "weights = weights/weights.sum() #normalise weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response on dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the ensemble to the whole of the development data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev = inputPipe.transform(devData[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(inData, ensemble, weights, n=-1): #Loop though each classifier and predict data class\n",
    "    pred = np.zeros((len(inData), 1))\n",
    "    if n == -1:\n",
    "        n = len(ensemble)+1\n",
    "    ensemble = ensemble[0:n] #Use only specified number of classifiers\n",
    "    weights = weights[0:n]\n",
    "    weights = weights/weights.sum() #Renormalise weights\n",
    "    \n",
    "    for i, model in enumerate(ensemble):\n",
    "        pred += weights[i] * model.predict(inData, verbose=0)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X_dev, ensemble, weights)\n",
    "devData['pred_class'] = pandas.Series(pred[:,0], index=devData.index) #Add predicted class to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devAUC = roc_auc_score(devData[\"gen_target\"].values.astype('int'), devData['pred_class'])\n",
    "print('Area under ROC curve for development data is {:.5f}'.format(devAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble testing\n",
    "What benefit does ensembling bring? Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ensembleSize):\n",
    "    auc = roc_auc_score(devData[\"gen_target\"].values.astype('int'),\n",
    "                        predict(X_dev, ensemble, weights, i+1))\n",
    "    if not i:\n",
    "        print(\"AUC using best classifier:\\t{:.5f}\".format(auc))\n",
    "    else:\n",
    "        print(\"AUC using {} classifiers:\\t{:.5f}\".format(i+1, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definite improvements seen, but the improvements will eventually saturate. The number to use and the ways the weights are calculated are hyperparameters of the MVA. We could have set aside some of the development data for tuning this, but for now we'll just stick with what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response on val data\n",
    "Having done all the development of the classifier, we're now ready to do final testing on the withheld validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = inputPipe.transform(valData[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X_val, ensemble, weights)\n",
    "valData['pred_class'] = pandas.Series(pred[:,0], index=valData.index) #Add predicted class to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valAUC = roc_auc_score(valData[\"gen_target\"].values.astype('int'), valData['pred_class'])\n",
    "print('Area under ROC curve for validation data is {:.5f}'.format(valAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble testing\n",
    "Again we can confirm that ensembling helps. **N.B.** Do not use the validation data for any tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ensembleSize):\n",
    "    auc = roc_auc_score(valData[\"gen_target\"].values.astype('int'),\n",
    "                              predict(X_val, ensemble, weights, i+1))\n",
    "    if not i:\n",
    "        print(\"AUC using best classifier:\\t{:.5f}\".format(auc))\n",
    "    else:\n",
    "        print(\"AUC using {} classifiers:\\t{:.5f}\".format(i+1, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "In the earlier evaluations of the ROC AUC we took the data altogether giving one evaluation of the ROC and no uncertainty. Instead we can sample the predictions with replacement and evaluate the ROC on the bootstrap samples. By doing this many times we can converge to a better estimation of true ROC AUC, and get an uncertainty.\n",
    "\n",
    "This takes a while so we'll use multithreading to evaluate both development and validation performance at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aucArgs = [{'labels':valData['gen_target'], 'preds':valData['pred_class'], \n",
    "            'name':'Val', 'indeces':valData.index.tolist()},\n",
    "           {'labels':devData['gen_target'], 'preds':devData['pred_class'], \n",
    "            'name':'Dev', 'indeces':devData.index.tolist()}]\n",
    "aucs = mpRun(aucArgs, rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanScores = {}\n",
    "\n",
    "for sample in ['Dev', 'Val']:\n",
    "    meanScores[sample] = (np.mean(aucs[sample]), np.std(aucs[sample])/np.sqrt(len(aucs[sample])))\n",
    "    print(sample + ' ROC AUC, Mean = {} +- {}'.format(meanScores[sample][0], meanScores[sample][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 8])\n",
    "plt.plot(*roc_curve(devData['gen_target'].values, devData['pred_class'].values)[:2],\n",
    "         label=r'Dev, $auc={:.6f}\\pm{:.6f}$'.format(meanScores['Dev'][0], meanScores['Dev'][1]),\n",
    "         linestyle='dashed', color='b')\n",
    "plt.plot(*roc_curve(valData['gen_target'].values, valData['pred_class'].values)[:2],\n",
    "         label=r'Val, $auc={:.5f}\\pm{:.5f}$'.format(meanScores['Val'][0], meanScores['Val'][1]),\n",
    "         color='b')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='No discrimination')\n",
    "plt.xlabel('Background acceptance', fontsize=24, color='black')\n",
    "plt.ylabel('Signal acceptance', fontsize=24, color='black')\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVA distribution\n",
    "We can also plot the distribution of the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'hist' : True, 'kde' : False, 'norm_hist' : True, 'bins' : np.linspace(0, 1, 50)}\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.distplot(valData[bkgVal]['pred_class'], label='Background', **params)\n",
    "sns.distplot(valData[sigVal]['pred_class'], label='Signal', **params)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel(\"Class prediction\", fontsize=24, color='black')\n",
    "plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{dp}$\", fontsize=24, color='black')\n",
    "plt.xlim([0,1])\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output classified data to ROOT TTree\n",
    "In case you require the classified data to be in ROOT format we can convert it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if useROOT:\n",
    "    array = np.array([tuple(x) for x in devData.values], dtype=[(x, np.float64) for x in devData.columns])\n",
    "    root_numpy.array2root(array, loc + 'classifiedDevData.root', mode='recreate')\n",
    "    array = np.array([tuple(x) for x in valData.values], dtype=[(x, np.float64) for x in valData.columns])\n",
    "    root_numpy.array2root(array, loc + 'classifiedValData.root', mode='recreate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/load\n",
    "We can save the classifier and load it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights/NN_fsFeatures_model0\n"
     ]
    }
   ],
   "source": [
    "name = \"weights/NN_{}_{}\".format(varSet, classModel)\n",
    "print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.system(\"mkdir weights\")\n",
    "os.system(\"rm \" + name + \"*.json\")\n",
    "os.system(\"rm \" + name + \"*.h5\")\n",
    "os.system(\"rm \" + name + \"*.pkl\")\n",
    "for i, model in enumerate(ensemble): #This is the other way of saving Keras models\n",
    "    json_string = model.to_json()\n",
    "    open(\"{0}_{1}.json\".format(name, i), 'wb').write(str.encode(json_string)) #Save layout as json\n",
    "    model.save_weights(name + '_' + str(i) + '.h5') #Save weights as h5\n",
    "with open(name + '_compile.pkl', 'w') as fout: #Save compile arguments; loaded model might need recompiling\n",
    "    json.dump(compileArgs, fout)\n",
    "with open(name + '_weights.pkl', 'wb') as fout: #Save weights for ensembling\n",
    "    cPickle.dump(weights, fout)\n",
    "with open(name + '_inputPipe.pkl', 'wb') as fout: #Save the pre-processing pipeline\n",
    "    cPickle.dump(inputPipe, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc1 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-075e98d88154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#    ensemble.append(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_weights.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_inputPipe.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0minputPipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc1 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "ensemble = []\n",
    "weights = None\n",
    "inputPipe = None\n",
    "compileArgs = None\n",
    "with open(name + '_compile.pkl', 'r') as fin:\n",
    "    compileArgs = json.load(fin)\n",
    "#for i in range(ensembleSize):\n",
    "#    model = model_from_json(open(name + '_' + str(i) + '.json').read(), )\n",
    "#    model.load_weights(name + \"_\" + str(i) + '.h5')\n",
    "#    model.compile(**compileArgs)\n",
    "#    ensemble.append(model)\n",
    "with open(name + '_weights.pkl', 'rb') as fin:\n",
    "    weights = cPickle.load(fin)\n",
    "with open(name + '_inputPipe.pkl', 'rb') as fin:\n",
    "    inputPipe = cPickle.load(fin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
