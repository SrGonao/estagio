{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B regressor final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division ,print_function\n",
    "import random\n",
    "import theano\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, GaussianNoise, BatchNormalization, Merge\n",
    "from keras.layers.advanced_activations import ELU, PReLU\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import theano.tensor as T\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy.misc\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "import sys\n",
    "sys.path.append('../../modules')\n",
    "from MPPlot import *\n",
    "from Processors import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "Here just looking at mu tau_h b b final-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples contains 10417 signal events and 168053 background events\n",
      "178470 events in total\n"
     ]
    }
   ],
   "source": [
    "mode = \"mu_tau_b_b\"\n",
    "loc = '../../../data_n/'\n",
    "SignalData = []\n",
    "BackgroundData = []\n",
    "\n",
    "energies = ['260', '270', '280', '300', '320', '400', '500', '550', '600' ,'700']\n",
    "backgrounds = ['TT' , 'WJetsToLNu']\n",
    "Data = np.concatenate((energies,backgrounds))\n",
    "\n",
    "def backgroundName(background):\n",
    "    if background == 'TT':\n",
    "        return loc + 'TT_TuneCUETP8M1_13TeV-powheg-pythia8_2.csv'\n",
    "    elif background == 'WJetsToLNu':\n",
    "        return loc + 'WJetsToLNu_TuneCUETP8M1_13TeV-amcatnloFXFX-pythia8.csv'\n",
    "\n",
    "\n",
    "\n",
    " #Load data from CSV files\n",
    "for energy in energies:\n",
    "    signaldata = pandas.read_csv(loc + \"GluGluToRadionToHHTo2B2Tau_M-\" + str(energy) +\"_narrow_13TeV-madgraph.csv\")\n",
    "    signaldata['gen_label'] = energy\n",
    "    SignalData.append(signaldata)\n",
    "\n",
    "\n",
    "for back in backgrounds:\n",
    "    backgroundData = pandas.read_csv(backgroundName(back))\n",
    "    backgroundData['gen_label'] = back\n",
    "    BackgroundData.append(backgroundData)    \n",
    "     \n",
    "    \n",
    "signalData = SignalData[0]\n",
    "for signal_data in SignalData[1:]:\n",
    "    signalData = signalData.append(signal_data, ignore_index=True)    \n",
    "signalData.drop([x for x in signalData.columns if 'Unnamed' in x], axis=1, inplace=True)    \n",
    "signalData['gen_target'] = pandas.Series(np.ones(signalData.size))\n",
    "backgroundData = BackgroundData[0]\n",
    "for background_data in BackgroundData[1:]:\n",
    "    backgroundData = backgroundData.append(background_data, ignore_index=True)    \n",
    "backgroundData.drop([x for x in backgroundData.columns if 'Unnamed' in x], axis=1, inplace=True)    \n",
    "backgroundData['gen_target'] = pandas.Series(np.zeros(backgroundData.size))\n",
    "\n",
    "print(\"Samples contains {0} signal events and {1} background events\".format(len(signalData), len(backgroundData)))\n",
    "print(\"{} events in total\".format(len(signalData)+len(backgroundData)))\n",
    "data = signalData.append(backgroundData, ignore_index = True) #Combine into signal dataset\n",
    "\n",
    "\n",
    "def abs_(x):\n",
    "    if type(x) is float:\n",
    "        return abs(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def cleanData(X):\n",
    "    \"\"\"\n",
    "    Recives data X\n",
    "    Returns: X after removing points that would fail to convert to float32 \n",
    "    \"\"\"\n",
    "    X.applymap(abs_)\n",
    "    \n",
    "    \n",
    "    over  = (X > np.finfo(np.float32).max)\n",
    "    under = (X < np.finfo(np.float32).min)\n",
    "    selecting = pandas.Series(np.zeros(len(X)), dtype=np.bool)\n",
    "\n",
    "    for label in over.columns:\n",
    "        if label != 'gen_label':\n",
    "            selecting = selecting | over[label] | under[label]\n",
    "    \n",
    "    \n",
    "    X = X[np.logical_not(selecting)].reset_index(drop=True)\n",
    "    \n",
    "    return X\n",
    "\n",
    "data = cleanData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data and add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "particles = ['t_0', 't_1', 'b_0', 'b_1', 'h_tt', 'h_bb', 'diH', 'gen_t_0', 'gen_t_1', 'gen_b_0' ,'gen_b_1']\n",
    "\n",
    "for p in particles:\n",
    "    moveToCartesian(data, p) #Move pT, eta, and phi to p_x, p_y, and p_z\n",
    "    if(not str.startswith(p, \"gen\")):\n",
    "        addEnergy(data, p) #Calculate energy and absolute momentum\n",
    "\n",
    "\n",
    "moveToCartesian(data, 'mPT', False)  #Move Missing pT and phi to p_x and p_y\n",
    "addAbsMom(data, 'mPT', False) #Calculate absolute missing transverse momentum\n",
    "addMT(data, data['t_1_pT'], data['t_1_phi'], 't_1') #Calculate transverse mass of tau_mu\n",
    "data['hl_mT'] = np.sqrt(2*data['t_1_pT']*data['mPT_pT']*(1-np.cos(deltaphi(data['t_1_phi'], data['mPT_phi']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3928 events for training, 174533 events for validation\n"
     ]
    }
   ],
   "source": [
    "sig_devIndeces, sig_valIndeces = \\\n",
    "                train_test_split([i for i in data[(data.gen_target == 1) & (data.gen_mctMatch == 1)].index.tolist()],\n",
    "                                 test_size=0.2, random_state=1337)\n",
    "\n",
    "    \n",
    "devData = data.loc[sig_devIndeces].copy()\n",
    "#devData = devData.append(data.loc[bkg_devIndeces].copy(), ignore_index = True)\n",
    "valData = data.loc[sig_valIndeces].copy()\n",
    "valData = valData.append(data[data.gen_target == 0].copy(), ignore_index = True)\n",
    "valData = valData.append(data[(data.gen_target == 1) & (data.gen_mctMatch == 0)].copy(), ignore_index = True)\n",
    "sig = (valData.gen_target == 1) & (valData.gen_mctMatch == 1)\n",
    "bkg = (valData.gen_target == 0)\n",
    "sigMM = (valData.gen_target == 1) & (valData.gen_mctMatch == 0)\n",
    "\n",
    "print(\"{} events for training, {} events for validation\".format(len(devData), len(valData)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0_pT , t_0_eta , t_0_phi , t_0_mass , t_1_pT , t_1_eta , t_1_phi , t_1_mass , b_0_pT , b_0_eta , b_0_phi , b_0_mass , b_0_csv , b_1_pT , b_1_eta , b_1_phi , b_1_mass , b_1_csv , mPT_pT , mPT_phi , h_tt_pT , h_tt_eta , h_tt_phi , h_tt_mass , h_tt_svFit_mass , h_bb_pT , h_bb_eta , h_bb_phi , h_bb_mass , diH_pT , diH_eta , diH_phi , diH_mass , diH_kinFit_mass , diH_kinFit_prob , mT , hT , sT , centrality , eVis , sphericity , spherocity , aplanarity , aplanority , upsilon , dShape , sphericityEigen0 , sphericityEigen1 , sphericityEigen2 , spherocityEigen0 , spherocityEigen1 , spherocityEigen2 , t_0_px , t_0_py , t_0_pz , t_0_|p| , t_0_E , t_1_px , t_1_py , t_1_pz , t_1_|p| , t_1_E , b_0_px , b_0_py , b_0_pz , b_0_|p| , b_0_E , b_1_px , b_1_py , b_1_pz , b_1_|p| , b_1_E , h_tt_px , h_tt_py , h_tt_pz , h_tt_|p| , h_tt_E , h_bb_px , h_bb_py , h_bb_pz , h_bb_|p| , h_bb_E , diH_px , diH_py , diH_pz , diH_|p| , diH_E , mPT_px , mPT_py , mPT_|p| , t_1_mT , hl_mT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "genFeatures = [gen for gen in data.columns if str.startswith(gen, \"gen\")]\n",
    "trainFeatures = [var for var in data.columns if var not in genFeatures]\n",
    "print(*trainFeatures, sep=' , ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_0_mass', 't_1_mass', 'b_0_mass', 'b_1_mass', 'h_tt_mass', 'h_bb_mass', 'diH_mass', 'mT', 'hT', 'sT', 'centrality', 'eVis', 'sphericity', 'spherocity', 'aplanarity', 'aplanority', 'upsilon', 'dShape', 'sphericityEigen0', 'sphericityEigen1', 'sphericityEigen2', 'spherocityEigen0', 'spherocityEigen1', 'spherocityEigen2', 't_0_px', 't_0_py', 't_0_pz', 't_0_|p|', 't_0_E', 't_1_px', 't_1_py', 't_1_pz', 't_1_|p|', 't_1_E', 'b_0_px', 'b_0_py', 'b_0_pz', 'b_0_|p|', 'b_0_E', 'b_1_px', 'b_1_py', 'b_1_pz', 'b_1_|p|', 'b_1_E', 'h_tt_px', 'h_tt_py', 'h_tt_pz', 'h_tt_|p|', 'h_tt_E', 'h_bb_px', 'h_bb_py', 'h_bb_pz', 'h_bb_|p|', 'h_bb_E', 'diH_px', 'diH_py', 'diH_pz', 'diH_|p|', 'diH_E', 'mPT_px', 'mPT_py', 'mPT_|p|', 't_1_mT', 'hl_mT']\n"
     ]
    }
   ],
   "source": [
    "pTEtaPhi = [var for var in trainFeatures for x in ['pT', 'eta', 'phi'] if x in var]\n",
    "\n",
    "other = [var for var in trainFeatures for x in ['Fit', 'csv'] if x in var]\n",
    "trainFeatures = [var for var in trainFeatures if var not in pTEtaPhi+ other]\n",
    "print(trainFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shapes = [var for var in trainFeatures for x in ['aplan', 'dShape', 'spher', 'upsilon'] if x in var]\n",
    "shapeFeatures = [var for var in trainFeatures if var in shapes]\n",
    "eventKinematicFeatures = ['centrality', 'eVis', 'hT', 'sT','mT']\n",
    "jetFeatures = [var for var in trainFeatures if 'Jet' in var and 'Jets' not in var]\n",
    "multiplicityFeatures = ['nBJets', 'nJets', 'nPhotons', 'nTauJets']\n",
    "hlFeatures = [var for var in trainFeatures if (str.startswith(var, \"hl_\"))]\n",
    "recoFeatures = [var for var in trainFeatures if (str.startswith(var, \"h_\")) or (str.startswith(var, \"diH_\"))]\n",
    "epFeatures = [var for var in trainFeatures if (str.endswith(var, \"_E\")) or (str.endswith(var, \"_|p|\"))]\n",
    "fsFeatures =  [var for var in trainFeatures if var not in shapeFeatures + pTEtaPhi + hlFeatures + recoFeatures ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalFeatures = [var for var in fsFeatures + recoFeatures if var not in pTEtaPhi] + ['hl_mT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ganTrainFeatures = finalFeatures\n",
    "normIn = True\n",
    "pca = True\n",
    "whiten = False\n",
    "nSplits = 10\n",
    "cvTests = True\n",
    "ensembleSize = 10\n",
    "ensembleMode = 'loss'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGenerator(nOut):\n",
    "    classModel = Sequential()\n",
    "    depth = 3\n",
    "    width = 100\n",
    "    classModel.add(Dense(width, input_dim=1)) #First layer requires number of inputs\n",
    "    classModel.add(Activation('selu'))  \n",
    "    for i in range(depth): #Continue to add hidden layers\n",
    "        classModel.add(Dense(width)) #Subsequent layers inherit input_dim from previous layer   \n",
    "        classModel.add(Activation('selu'))       \n",
    "    classModel.add(Dense(nOut, activation='linear')) #Final layer requires one output\n",
    "    #classModel.compile(**compileArgs) #Compile the network graph to prepare it for use\n",
    "    #classModel.summary()\n",
    "    return classModel\n",
    "\n",
    "def getDiscriminator(nIn):\n",
    "    classModel = Sequential()\n",
    "    depth = 3\n",
    "    width = 100\n",
    "    classModel.add(Dense(width, input_dim=nIn)) #First layer requires number of inputs\n",
    "    classModel.add(Activation('selu'))\n",
    "    for i in range(depth): #Continue to add hidden layers\n",
    "        classModel.add(Dense(width)) #Subsequent layers inherit input_dim from previous layer \n",
    "        classModel.add(Activation('selu'))      \n",
    "    classModel.add(Dense(1, activation='sigmoid')) #Final layer requires one output\n",
    "    #classModel.summary()\n",
    "    return classModel\n",
    "\n",
    "def getDm(nIn):\n",
    "    compileArgs = {'loss':'binary_crossentropy','optimizer':'adam'} #Loss function and optimiser for NN\n",
    "    classModel = Sequential()\n",
    "    classModel.add(getDiscriminator(nIn))\n",
    "    classModel.compile(**compileArgs) #Compile the network graph to prepare it for use\n",
    "    #classModel.summary()\n",
    "    return classModel\n",
    "\n",
    "def getAm(nIn,nOut):\n",
    "    compileArgs = {'loss':'binary_crossentropy','optimizer':'adam'} #Loss function and optimiser for NN\n",
    "    classModel = Sequential()\n",
    "    classModel.add(getGenerator(nOut))\n",
    "    classModel.add(getDiscriminator(nIn))\n",
    "    classModel.compile(**compileArgs) #Compile the network graph to prepare it for use\n",
    "    #classModel.summary()\n",
    "    return classModel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepsIn = []\n",
    "if not normIn and not pca:\n",
    "    stepsIn.append(('ident', StandardScaler(with_mean=False, with_std=False))) #For compatability\n",
    "else:\n",
    "    if normIn:\n",
    "        stepsIn.append(('normIn', StandardScaler()))\n",
    "    if pca:\n",
    "        stepsIn.append(('pca', PCA(whiten=whiten)))\n",
    "inputPipe = Pipeline(stepsIn)\n",
    "stepsOut = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class = inputPipe.fit_transform(devData[finalFeatures].values.astype(theano.config.floatX))\n",
    "Y_class = pandas.Series(np.ones(X_class.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train regressors\n",
    "Train nSplit times to find best convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData = (None, None)\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {}\n",
    "        self.losses['loss'] = []\n",
    "        self.losses['val_loss'] = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses['loss'].append(self.model.evaluate(trainingData[0], trainingData[1], verbose=0))\n",
    "        self.losses['val_loss'].append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainParams = {'batch_size' : 64, 'verbose' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1.0\n",
      "1       1.0\n",
      "2       1.0\n",
      "3       1.0\n",
      "4       1.0\n",
      "5       1.0\n",
      "6       1.0\n",
      "7       1.0\n",
      "8       1.0\n",
      "9       1.0\n",
      "10      1.0\n",
      "11      1.0\n",
      "12      1.0\n",
      "13      1.0\n",
      "14      1.0\n",
      "15      1.0\n",
      "16      1.0\n",
      "17      1.0\n",
      "18      1.0\n",
      "19      1.0\n",
      "20      1.0\n",
      "21      1.0\n",
      "22      1.0\n",
      "23      1.0\n",
      "24      1.0\n",
      "25      1.0\n",
      "26      1.0\n",
      "27      1.0\n",
      "28      1.0\n",
      "29      1.0\n",
      "       ... \n",
      "7826    0.0\n",
      "7827    0.0\n",
      "7828    0.0\n",
      "7829    0.0\n",
      "7830    0.0\n",
      "7831    0.0\n",
      "7832    0.0\n",
      "7833    0.0\n",
      "7834    0.0\n",
      "7835    0.0\n",
      "7836    0.0\n",
      "7837    0.0\n",
      "7838    0.0\n",
      "7839    0.0\n",
      "7840    0.0\n",
      "7841    0.0\n",
      "7842    0.0\n",
      "7843    0.0\n",
      "7844    0.0\n",
      "7845    0.0\n",
      "7846    0.0\n",
      "7847    0.0\n",
      "7848    0.0\n",
      "7849    0.0\n",
      "7850    0.0\n",
      "7851    0.0\n",
      "7852    0.0\n",
      "7853    0.0\n",
      "7854    0.0\n",
      "7855    0.0\n",
      "Length: 7856, dtype: float64\n",
      "Train on 5263 samples, validate on 2593 samples\n",
      "Epoch 1/5\n",
      "5263/5263 [==============================] - 0s - loss: 0.1367 - val_loss: 0.0128\n",
      "Epoch 2/5\n",
      "5263/5263 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0027\n",
      "Epoch 3/5\n",
      "5263/5263 [==============================] - 0s - loss: 9.5616e-04 - val_loss: 0.0014\n",
      "Epoch 4/5\n",
      "5263/5263 [==============================] - 0s - loss: 3.8874e-04 - val_loss: 0.0011\n",
      "Epoch 5/5\n",
      "5263/5263 [==============================] - 0s - loss: 2.2903e-04 - val_loss: 8.5089e-04\n",
      "Score is: {'loss': 0.00085089458626724475}\n",
      "Train on 2631 samples, validate on 1297 samples\n",
      "Epoch 1/1\n",
      "2368/2631 [==========================>...] - ETA: 0s - loss: 0.0050"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_16_input to have shape (None, 1) but got array with shape (5263, 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b8bc52e0a7f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     A.fit(faketrain,ytrain , validation_data = (faketest, ytest),\n\u001b[1;32m     43\u001b[0m               \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearlyStop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveBest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossHistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m               epochs= 1)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/goncalo/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/goncalo/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/goncalo/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/goncalo/anaconda2/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-63880b642e70>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/goncalo/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m    890\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/goncalo/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1465\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/goncalo/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1232\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1235\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1236\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/goncalo/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_16_input to have shape (None, 1) but got array with shape (5263, 52)"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "results = []\n",
    "histories = []\n",
    "os.system(\"rm train_weights/*.h5\")\n",
    "os.system(\"rm train_weights/*.json\")\n",
    "os.system(\"rm train_weights/*.pkl\")\n",
    "G = getGenerator(len(finalFeatures))\n",
    "\n",
    "for i in range(400):   \n",
    "    fakeData = []\n",
    "    fakeInput = []\n",
    "    for i in range(len(devData)):\n",
    "        fakeInput.append(random.random())\n",
    "    fakeData = G.predict(fakeInput)\n",
    "    fakeData = pandas.DataFrame(data=fakeData,    # values\n",
    "                 columns=finalFeatures)  # 1st row as the column names\n",
    "    X_fake = inputPipe.fit_transform(fakeData.values.astype(theano.config.floatX))\n",
    "    X_all = np.append(X_fake, X_class,axis = 0)\n",
    "    y_class = Y_class.append(pandas.Series(np.zeros(X_fake.shape[0])), ignore_index = True)\n",
    "    print(y_class)\n",
    "    \n",
    "    faketrain,faketest,ytrain,ytest    = train_test_split(fakeInput, Y_class, test_size=0.33, random_state=42) \n",
    "    Xtrain, Xtest, Ytrain,Ytest    = train_test_split(X_all, y_class, test_size=0.33, random_state=42)\n",
    "    D = None # Clearing the NN\n",
    "    D = getDm(len(finalFeatures))\n",
    "    trainingData = (Xtrain, Ytrain)\n",
    "    lossHistory = LossHistory()\n",
    "    earlyStop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "    saveBest = ModelCheckpoint(\"train_weights/Dbest.h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "    D.fit(Xtrain, Ytrain,\n",
    "              validation_data = (Xtest, Ytest),\n",
    "              callbacks = [earlyStop,saveBest, lossHistory],\n",
    "              epochs = 5)\n",
    "    histories.append(lossHistory.losses)\n",
    "    D.load_weights(\"train_weights/Dbest.h5\")\n",
    "    results.append({})\n",
    "    results[-1]['loss'] = D.evaluate(Xtest, Ytest, verbose=0)\n",
    "    print (\"Score is:\", results[-1])\n",
    "    D.save('train_weights/Dtrain_' + str(i-1) + '.h5')   \n",
    "    A = getAm(len(finalFeatures),len(finalFeatures))\n",
    "    \n",
    "    A.fit(faketrain,ytrain , validation_data = (faketest, ytest),\n",
    "              callbacks = [earlyStop,saveBest, lossHistory],\n",
    "              epochs= 1)\n",
    "\n",
    "        \n",
    "with open('train_weights/resultsFile.pkl', 'wb') as fout: \n",
    "    pickle.dump(results, fout)\n",
    "print (\"Cross-validation took {:.3f}s \".format(time.time() - start))\n",
    "X_reg = None\n",
    "y_reg = None\n",
    "train = None\n",
    "test = None\n",
    "D.summary()\n",
    "D = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "if cvTests:\n",
    "    for i, history in enumerate(histories):\n",
    "        if i == 0:\n",
    "            plt.plot(history['loss'], color='g', label='Training')\n",
    "            plt.plot(history['val_loss'], color='b', label='Testing')\n",
    "        else:\n",
    "            plt.plot(history['loss'], color='g')\n",
    "            plt.plot(history['val_loss'], color='b')\n",
    "    plt.legend(fontsize=16)\n",
    "else:\n",
    "    for history in histories:\n",
    "        plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"Epoch\", fontsize=24, color='black')\n",
    "plt.ylabel(\"MSE\", fontsize=24, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"weights/NN_B_Regressor_App_\" + mode + \"_\" \n",
    "print (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# added 3 'b's (?) for some reason some need and other don't... don't get it\n",
    "# i guess json as str and the others are bin\n",
    "\n",
    "os.system(\"rm \" + name + \"*.json\")\n",
    "os.system(\"rm \" + name + \"*.h5\")\n",
    "os.system(\"rm \" + name + \"*.pkl\")\n",
    "for i, model in enumerate(ensemble):\n",
    "    json_string = model.to_json()\n",
    "    open(name + '_' + str(i) + '.json', 'w').write(json_string) \n",
    "    model.save_weights(name + '_' + str(i) + '.h5')\n",
    "with open(name + '_compile.json', 'w') as fout:\n",
    "    json.dump(compileArgs, fout)\n",
    "with open(name + '_weights.pkl', 'wb') as fout:\n",
    "    pickle.dump(weights, fout)\n",
    "with open(name + '_inputPipe.pkl', 'wb') as fout:\n",
    "    pickle.dump(inputPipe, fout)\n",
    "with open(name + '_outputPipe.pkl', 'wb') as fout:\n",
    "    pickle.dump(outputPipe, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response of ensemble on development data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "weights = None\n",
    "inputPipe = None\n",
    "outputPipe = None\n",
    "compileArgs = None\n",
    "with open(name + '_compile.json', 'r') as fin:\n",
    "    compileArgs = json.load(fin)\n",
    "for i in range(ensembleSize):\n",
    "    model = model_from_json(open(name + '_' + str(i) + '.json').read())\n",
    "    model.load_weights(name + \"_\" + str(i) + '.h5')\n",
    "    model.compile(**compileArgs)\n",
    "    ensemble.append(model)\n",
    "with open(name + '_weights.pkl', 'rb') as fin:\n",
    "    weights = pickle.load(fin)\n",
    "with open(name + '_inputPipe.pkl', 'rb') as fin:\n",
    "    inputPipe = pickle.load(fin)\n",
    "with open(name + '_outputPipe.pkl', 'rb') as fin:\n",
    "    outputPipe = pickle.load(fin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
