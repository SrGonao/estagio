{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B regressor final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import random\n",
    "import theano\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, GaussianNoise, BatchNormalization, Merge\n",
    "from keras.layers.advanced_activations import ELU, PReLU\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import theano.tensor as T\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy.misc\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "import sys\n",
    "sys.path.append('../../modules')\n",
    "from MPPlot import *\n",
    "from Processors import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "Here just looking at mu tau_h b b final-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples contains 10417 signal events and 168053 background events\n",
      "178470 events in total\n"
     ]
    }
   ],
   "source": [
    "mode = \"mu_tau_b_b\"\n",
    "loc = '../../../data_n/'\n",
    "SignalData = []\n",
    "BackgroundData = []\n",
    "\n",
    "energies = ['260', '270', '280', '300', '320', '400', '500', '550', '600' ,'700']\n",
    "backgrounds = ['TT' , 'WJetsToLNu']\n",
    "Data = np.concatenate((energies,backgrounds))\n",
    "\n",
    "def backgroundName(background):\n",
    "    if background == 'TT':\n",
    "        return loc + 'TT_TuneCUETP8M1_13TeV-powheg-pythia8_2.csv'\n",
    "    elif background == 'WJetsToLNu':\n",
    "        return loc + 'WJetsToLNu_TuneCUETP8M1_13TeV-amcatnloFXFX-pythia8.csv'\n",
    "\n",
    "\n",
    "\n",
    " #Load data from CSV files\n",
    "for energy in energies:\n",
    "    signaldata = pandas.read_csv(loc + \"GluGluToRadionToHHTo2B2Tau_M-\" + str(energy) +\"_narrow_13TeV-madgraph.csv\")\n",
    "    signaldata['gen_label'] = energy\n",
    "    SignalData.append(signaldata)\n",
    "\n",
    "\n",
    "for back in backgrounds:\n",
    "    backgroundData = pandas.read_csv(backgroundName(back))\n",
    "    backgroundData['gen_label'] = back\n",
    "    BackgroundData.append(backgroundData)    \n",
    "     \n",
    "    \n",
    "signalData = SignalData[0]\n",
    "for signal_data in SignalData[1:]:\n",
    "    signalData = signalData.append(signal_data, ignore_index=True)    \n",
    "signalData.drop([x for x in signalData.columns if 'Unnamed' in x], axis=1, inplace=True)    \n",
    "signalData['gen_target'] = pandas.Series(np.ones(signalData.size))\n",
    "backgroundData = BackgroundData[0]\n",
    "for background_data in BackgroundData[1:]:\n",
    "    backgroundData = backgroundData.append(background_data, ignore_index=True)    \n",
    "backgroundData.drop([x for x in backgroundData.columns if 'Unnamed' in x], axis=1, inplace=True)    \n",
    "backgroundData['gen_target'] = pandas.Series(np.zeros(backgroundData.size))\n",
    "\n",
    "print(\"Samples contains {0} signal events and {1} background events\".format(len(signalData), len(backgroundData)))\n",
    "print(\"{} events in total\".format(len(signalData)+len(backgroundData)))\n",
    "data = signalData.append(backgroundData, ignore_index = True) #Combine into signal dataset\n",
    "\n",
    "\n",
    "def abs_(x):\n",
    "    if type(x) is float:\n",
    "        return abs(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def cleanData(X):\n",
    "    \"\"\"\n",
    "    Recives data X\n",
    "    Returns: X after removing points that would fail to convert to float32 \n",
    "    \"\"\"\n",
    "    X.applymap(abs_)\n",
    "    \n",
    "    \n",
    "    over  = (X > np.finfo(np.float32).max)\n",
    "    under = (X < np.finfo(np.float32).min)\n",
    "    selecting = pandas.Series(np.zeros(len(X)), dtype=np.bool)\n",
    "\n",
    "    for label in over.columns:\n",
    "        if label != 'gen_label':\n",
    "            selecting = selecting | over[label] | under[label]\n",
    "    \n",
    "    \n",
    "    X = X[np.logical_not(selecting)].reset_index(drop=True)\n",
    "    \n",
    "    return X\n",
    "\n",
    "data = cleanData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data and add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "particles = ['t_0', 't_1', 'b_0', 'b_1', 'h_tt', 'h_bb', 'diH', 'gen_t_0', 'gen_t_1', 'gen_b_0' ,'gen_b_1']\n",
    "\n",
    "for p in particles:\n",
    "    moveToCartesian(data, p) #Move pT, eta, and phi to p_x, p_y, and p_z\n",
    "    if(not str.startswith(p, \"gen\")):\n",
    "        addEnergy(data, p) #Calculate energy and absolute momentum\n",
    "\n",
    "\n",
    "moveToCartesian(data, 'mPT', False)  #Move Missing pT and phi to p_x and p_y\n",
    "addAbsMom(data, 'mPT', False) #Calculate absolute missing transverse momentum\n",
    "addMT(data, data['t_1_pT'], data['t_1_phi'], 't_1') #Calculate transverse mass of tau_mu\n",
    "data['hl_mT'] = np.sqrt(2*data['t_1_pT']*data['mPT_pT']*(1-np.cos(deltaphi(data['t_1_phi'], data['mPT_phi']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3928 events for training, 174533 events for validation\n"
     ]
    }
   ],
   "source": [
    "sig_devIndeces, sig_valIndeces = \\\n",
    "                train_test_split([i for i in data[(data.gen_target == 1) & (data.gen_mctMatch == 1)].index.tolist()],\n",
    "                                 test_size=0.2, random_state=1337)\n",
    "\n",
    "    \n",
    "devData = data.loc[sig_devIndeces].copy()\n",
    "#devData = devData.append(data.loc[bkg_devIndeces].copy(), ignore_index = True)\n",
    "valData = data.loc[sig_valIndeces].copy()\n",
    "valData = valData.append(data[data.gen_target == 0].copy(), ignore_index = True)\n",
    "valData = valData.append(data[(data.gen_target == 1) & (data.gen_mctMatch == 0)].copy(), ignore_index = True)\n",
    "sig = (valData.gen_target == 1) & (valData.gen_mctMatch == 1)\n",
    "bkg = (valData.gen_target == 0)\n",
    "sigMM = (valData.gen_target == 1) & (valData.gen_mctMatch == 0)\n",
    "\n",
    "print(\"{} events for training, {} events for validation\".format(len(devData), len(valData)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0_pT , t_0_eta , t_0_phi , t_0_mass , t_1_pT , t_1_eta , t_1_phi , t_1_mass , b_0_pT , b_0_eta , b_0_phi , b_0_mass , b_0_csv , b_1_pT , b_1_eta , b_1_phi , b_1_mass , b_1_csv , mPT_pT , mPT_phi , h_tt_pT , h_tt_eta , h_tt_phi , h_tt_mass , h_tt_svFit_mass , h_bb_pT , h_bb_eta , h_bb_phi , h_bb_mass , diH_pT , diH_eta , diH_phi , diH_mass , diH_kinFit_mass , diH_kinFit_prob , mT , hT , sT , centrality , eVis , sphericity , spherocity , aplanarity , aplanority , upsilon , dShape , sphericityEigen0 , sphericityEigen1 , sphericityEigen2 , spherocityEigen0 , spherocityEigen1 , spherocityEigen2 , t_0_px , t_0_py , t_0_pz , t_0_|p| , t_0_E , t_1_px , t_1_py , t_1_pz , t_1_|p| , t_1_E , b_0_px , b_0_py , b_0_pz , b_0_|p| , b_0_E , b_1_px , b_1_py , b_1_pz , b_1_|p| , b_1_E , h_tt_px , h_tt_py , h_tt_pz , h_tt_|p| , h_tt_E , h_bb_px , h_bb_py , h_bb_pz , h_bb_|p| , h_bb_E , diH_px , diH_py , diH_pz , diH_|p| , diH_E , mPT_px , mPT_py , mPT_|p| , t_1_mT , hl_mT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "genFeatures = [gen for gen in data.columns if str.startswith(gen, \"gen\")]\n",
    "trainFeatures = [var for var in data.columns if var not in genFeatures]\n",
    "print(*trainFeatures, sep=' , ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_0_mass', 't_1_mass', 'b_0_mass', 'b_1_mass', 'h_tt_mass', 'h_bb_mass', 'diH_mass', 'mT', 'hT', 'sT', 'centrality', 'eVis', 'sphericity', 'spherocity', 'aplanarity', 'aplanority', 'upsilon', 'dShape', 'sphericityEigen0', 'sphericityEigen1', 'sphericityEigen2', 'spherocityEigen0', 'spherocityEigen1', 'spherocityEigen2', 't_0_px', 't_0_py', 't_0_pz', 't_0_|p|', 't_0_E', 't_1_px', 't_1_py', 't_1_pz', 't_1_|p|', 't_1_E', 'b_0_px', 'b_0_py', 'b_0_pz', 'b_0_|p|', 'b_0_E', 'b_1_px', 'b_1_py', 'b_1_pz', 'b_1_|p|', 'b_1_E', 'h_tt_px', 'h_tt_py', 'h_tt_pz', 'h_tt_|p|', 'h_tt_E', 'h_bb_px', 'h_bb_py', 'h_bb_pz', 'h_bb_|p|', 'h_bb_E', 'diH_px', 'diH_py', 'diH_pz', 'diH_|p|', 'diH_E', 'mPT_px', 'mPT_py', 'mPT_|p|', 't_1_mT', 'hl_mT']\n"
     ]
    }
   ],
   "source": [
    "pTEtaPhi = [var for var in trainFeatures for x in ['pT', 'eta', 'phi'] if x in var]\n",
    "\n",
    "other = [var for var in trainFeatures for x in ['Fit', 'csv'] if x in var]\n",
    "trainFeatures = [var for var in trainFeatures if var not in pTEtaPhi+ other]\n",
    "print(trainFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shapes = [var for var in trainFeatures for x in ['aplan', 'dShape', 'spher', 'upsilon'] if x in var]\n",
    "shapeFeatures = [var for var in trainFeatures if var in shapes]\n",
    "eventKinematicFeatures = ['centrality', 'eVis', 'hT', 'sT','mT']\n",
    "jetFeatures = [var for var in trainFeatures if 'Jet' in var and 'Jets' not in var]\n",
    "multiplicityFeatures = ['nBJets', 'nJets', 'nPhotons', 'nTauJets']\n",
    "hlFeatures = [var for var in trainFeatures if (str.startswith(var, \"hl_\"))]\n",
    "recoFeatures = [var for var in trainFeatures if (str.startswith(var, \"h_\")) or (str.startswith(var, \"diH_\"))]\n",
    "epFeatures = [var for var in trainFeatures if (str.endswith(var, \"_E\")) or (str.endswith(var, \"_|p|\"))]\n",
    "fsFeatures =  [var for var in trainFeatures if var not in shapeFeatures + pTEtaPhi + hlFeatures + recoFeatures ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalFeatures = [var for var in fsFeatures + recoFeatures if var not in pTEtaPhi] + ['hl_mT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ganTrainFeatures = finalFeatures\n",
    "normIn = True\n",
    "pca = True\n",
    "whiten = False\n",
    "nSplits = 10\n",
    "cvTests = True\n",
    "ensembleSize = 10\n",
    "ensembleMode = 'loss'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGenerator(nOut):\n",
    "    classModel = Sequential()\n",
    "    depth = 3\n",
    "    width = 100\n",
    "    classModel.add(Dense(width, input_dim=1)) #First layer requires number of inputs\n",
    "    classModel.add(Activation('selu'))\n",
    "    for i in range(depth): #Continue to add hidden layers\n",
    "        classModel.add(Dense(width)) #Subsequent layers inherit input_dim from previous layer   \n",
    "        classModel.add(Activation('selu'))       \n",
    "    classModel.add(Dense(nOut, activation='linear')) #Final layer requires one output\n",
    "    #classModel.compile(**compileArgs) #Compile the network graph to prepare it for use\n",
    "    print('generator')\n",
    "    classModel.summary()\n",
    "    return classModel\n",
    "\n",
    "def getDiscriminator(nIn):\n",
    "    classModel = Sequential()\n",
    "    depth = 3\n",
    "    width = 100\n",
    "    classModel.add(Dense(width, input_dim=nIn)) #First layer requires number of inputs\n",
    "    classModel.add(Activation('selu'))\n",
    "    for i in range(depth): #Continue to add hidden layers\n",
    "        classModel.add(Dense(width)) #Subsequent layers inherit input_dim from previous layer \n",
    "        classModel.add(Activation('selu'))      \n",
    "    classModel.add(Dense(1, activation='sigmoid')) #Final layer requires one output\n",
    "    print('discriminator')\n",
    "    classModel.summary()\n",
    "    return classModel\n",
    "\n",
    "def getDm(nIn):\n",
    "    compileArgs = {'loss':'binary_crossentropy','optimizer':'adam'} #Loss function and optimiser for NN\n",
    "    classModel = Sequential()\n",
    "    classModel.add(getDiscriminator(nIn))\n",
    "    classModel.compile(**compileArgs) #Compile the network graph to prepare it for use\n",
    "    print('dm')\n",
    "    classModel.summary()\n",
    "    return classModel\n",
    "\n",
    "def getAm(nIn,nOut):\n",
    "    compileArgs = {'loss':'binary_crossentropy','optimizer':'adam'} #Loss function and optimiser for NN\n",
    "    classModel = Sequential()\n",
    "    classModel.add(getGenerator(nOut))\n",
    "    classModel.add(getDiscriminator(nIn))\n",
    "    classModel.compile(**compileArgs) #Compile the network graph to prepare it for use\n",
    "    print('Am')\n",
    "    classModel.summary()\n",
    "    return classModel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Merge, Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "compileArgs = {'loss':'binary_crossentropy','optimizer':'adam'}\n",
    "\n",
    "#g = getGenerator(len(finalFeatures))\n",
    "#d = getDiscriminator(len(finalFeatures))\n",
    "inputs1 = Input(shape=(1,))\n",
    "inputs2 = Input(shape=(len(finalFeatures),))\n",
    "\n",
    "Gx = Dense(20, activation=\"tanh\")(inputs1)\n",
    "Gx = Dense(20, activation=\"relu\")(Gx)\n",
    "Gx = Dense(len(finalFeatures), activation=\"linear\")(Gx)\n",
    "\n",
    "G = Model(inputs=[inputs1], outputs=[Gx])\n",
    "\n",
    "Dx = Dense(20, activation=\"tanh\")(inputs2)\n",
    "Dx = Dense(20, activation=\"relu\")(Dx)\n",
    "Dx = Dense(1, activation=\"sigmoid\")(Dx)\n",
    "\n",
    "d = Model(inputs=[inputs2], outputs=[Dx])\n",
    "\n",
    "D = Model(inputs=[inputs2], outputs=[d(inputs2)])\n",
    "D.compile(**compileArgs)\n",
    "\n",
    "temp = d(G(inputs1))\n",
    "\n",
    "d.trainable = False\n",
    "\n",
    "A = Model(inputs=[inputs1], outputs=[temp])\n",
    "A.compile(**compileArgs)\n",
    "\n",
    "#G = Model(inputs=[inputs1], outputs=g(inputs1))\n",
    "#G.compile(**compileArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepsIn = []\n",
    "if not normIn and not pca:\n",
    "    stepsIn.append(('ident', StandardScaler(with_mean=False, with_std=False))) #For compatability\n",
    "else:\n",
    "    if normIn:\n",
    "        stepsIn.append(('normIn', StandardScaler()))\n",
    "    if pca:\n",
    "        stepsIn.append(('pca', PCA(whiten=whiten)))\n",
    "inputPipe = Pipeline(stepsIn)\n",
    "stepsOut = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_class = inputPipe.fit_transform(devData[finalFeatures].values.astype(theano.config.floatX))\n",
    "Y_class = pandas.Series(np.ones(X_class.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train regressors\n",
    "Train nSplit times to find best convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData = (None, None)\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {}\n",
    "        self.losses['loss'] = []\n",
    "        self.losses['val_loss'] = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses['loss'].append(self.model.evaluate(trainingData[0], trainingData[1], verbose=0))\n",
    "        self.losses['val_loss'].append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainParams = {'batch_size' : 64, 'verbose' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5263 samples, validate on 2593 samples\n",
      "Epoch 1/10\n",
      "5263/5263 [==============================] - 0s - loss: 0.4283 - val_loss: 0.2757\n",
      "Epoch 2/10\n",
      "5263/5263 [==============================] - 0s - loss: 0.1757 - val_loss: 0.1281\n",
      "Epoch 3/10\n",
      "5263/5263 [==============================] - 0s - loss: 0.0828 - val_loss: 0.0699\n",
      "Epoch 4/10\n",
      "5263/5263 [==============================] - 0s - loss: 0.0463 - val_loss: 0.0434\n",
      "Epoch 5/10\n",
      "5263/5263 [==============================] - 0s - loss: 0.0293 - val_loss: 0.0295\n",
      "Epoch 6/10\n",
      "5263/5263 [==============================] - 0s - loss: 0.0198 - val_loss: 0.0230\n",
      "Epoch 7/10\n",
      "5263/5263 [==============================] - 0s - loss: 0.0142 - val_loss: 0.0172\n",
      "Epoch 8/10\n",
      "5263/5263 [==============================] - 0s - loss: 0.0105 - val_loss: 0.0144\n",
      "Epoch 9/10\n",
      "5263/5263 [==============================] - 0s - loss: 0.0080 - val_loss: 0.0116\n",
      "Epoch 10/10\n",
      "5263/5263 [==============================] - 0s - loss: 0.0062 - val_loss: 0.0102\n",
      "Score is: {'loss': 0.010151306988128944}\n",
      "Cross-validation took 2.196s \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 52)                0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 1)                 1501      \n",
      "=================================================================\n",
      "Total params: 1,501\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,501\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "results = []\n",
    "histories = []\n",
    "os.system(\"rm train_weights/*.h5\")\n",
    "os.system(\"rm train_weights/*.json\")\n",
    "os.system(\"rm train_weights/*.pkl\")\n",
    "#G = getGenerator(len(finalFeatures))\n",
    "\n",
    "for i in range(10):   \n",
    "    fakeData = []\n",
    "    #fakeInput = []\n",
    "    #for i in range(len(devData)):\n",
    "    #    fakeInput.append(random.random())\n",
    "    fakeInput = np.random.normal(size=len(devData))\n",
    "    fakeData = G.predict(fakeInput)\n",
    "    fakeData = pandas.DataFrame(data=fakeData,    # values\n",
    "                 columns=finalFeatures)  # 1st row as the column names\n",
    "    X_fake = inputPipe.fit_transform(fakeData.values.astype(theano.config.floatX))\n",
    "    X_all = np.append(X_fake, X_class,axis = 0)\n",
    "    y_class = Y_class.append(pandas.Series(np.zeros(X_fake.shape[0])), ignore_index = True)\n",
    "    #print(y_class)\n",
    "    \n",
    "    faketrain,faketest,ytrain,ytest    = train_test_split(fakeInput, Y_class, test_size=0.33, random_state=42) \n",
    "    Xtrain, Xtest, Ytrain,Ytest    = train_test_split(X_all, y_class, test_size=0.33, random_state=42)\n",
    "    #D = None # Clearing the NN\n",
    "    #D = getDm(len(finalFeatures))\n",
    "    \n",
    "    for var in (Ytrain, Ytest, ytrain, ytest):\n",
    "        var.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    trainingData = (Xtrain, Ytrain)\n",
    "    lossHistory = LossHistory()\n",
    "    earlyStop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "    saveBest = ModelCheckpoint(\"train_weights/Dbest.h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "    \n",
    "    #D.trainable = True\n",
    "    #G.trainable = False\n",
    "    \n",
    "    D.fit(Xtrain, Ytrain,\n",
    "              validation_data = (Xtest, Ytest),\n",
    "              callbacks = [earlyStop,saveBest, lossHistory],\n",
    "              epochs = 10)\n",
    "    histories.append(lossHistory.losses)\n",
    "    D.load_weights(\"train_weights/Dbest.h5\")\n",
    "    results.append({})\n",
    "    results[-1]['loss'] = D.evaluate(Xtest, Ytest, verbose=0)\n",
    "    print (\"Score is:\", results[-1])\n",
    "    #D.save('train_weights/Dtrain_' + str(i-1) + '.h5')   \n",
    "    #A = getAm(len(finalFeatures),len(finalFeatures))\n",
    "    \n",
    "    #print(ytrain, ytest)\n",
    "    \n",
    "    #D.trainable = False\n",
    "    #G.trainable = True\n",
    "    \n",
    "    #A.fit(faketrain, ytrain , validation_data = (faketest, ytest),\n",
    "    #          callbacks = [earlyStop, saveBest],\n",
    "    #          epochs= 1)\n",
    "\n",
    "        \n",
    "with open('train_weights/resultsFile.pkl', 'wb') as fout: \n",
    "    pickle.dump(results, fout)\n",
    "print (\"Cross-validation took {:.3f}s \".format(time.time() - start))\n",
    "X_reg = None\n",
    "y_reg = None\n",
    "train = None\n",
    "test = None\n",
    "D.summary()\n",
    "#D = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.63406374,  1.62377371,  1.11438815, ..., -0.82249101,\n",
       "         0.61739389,  0.55949123],\n",
       "       [ 1.76515658, -0.38221493, -1.14450292, ..., -0.18579312,\n",
       "         0.75341061,  0.23152016],\n",
       "       [-0.52286329,  1.04191002,  0.4768216 , ..., -0.47032915,\n",
       "        -1.03410982,  0.93825878],\n",
       "       [ 1.21466333,  2.44027469, -1.39607025, ..., -0.21209916,\n",
       "        -0.12952617,  2.64656484],\n",
       "       [ 0.64072572,  0.20214712, -0.55511502, ...,  0.50515741,\n",
       "        -1.17833128, -0.08803809]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(size=(5,len(devData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.38655351e-02,   4.28086400e-01,  -1.31487668e-01,\n",
       "          4.87975061e-01,  -6.99178696e-01,  -3.06481868e-01,\n",
       "          1.06860608e-01,   2.74074942e-01,  -5.03771722e-01,\n",
       "         -7.04815984e-02,  -4.46947753e-01,  -1.01619728e-01,\n",
       "         -1.36413530e-01,   6.53507710e-01,   1.07038140e-01,\n",
       "         -1.29809767e-01,   7.48746693e-02,   8.01183507e-02,\n",
       "          1.01078168e-01,   1.03857607e-01,  -4.94057834e-02,\n",
       "          7.66966864e-02,   3.58628511e-01,  -2.80939519e-01,\n",
       "          2.14428276e-01,   1.84830546e-01,   2.68296361e-01,\n",
       "         -3.45972568e-01,   7.90730491e-02,  -4.59765255e-01,\n",
       "         -6.07790172e-01,   3.81934941e-02,  -4.88776118e-01,\n",
       "          2.29948372e-01,  -3.40249725e-02,   1.55914938e-02,\n",
       "         -5.09830229e-02,   2.01460734e-01,   8.28693155e-03,\n",
       "         -2.02966798e-02,  -2.32114419e-01,   3.39686573e-01,\n",
       "         -2.93004870e-01,   7.75721669e-02,  -1.74755082e-01,\n",
       "         -2.55037807e-02,  -1.55610874e-01,   9.08224955e-02,\n",
       "          4.51401353e-01,   4.43069339e-01,  -2.61113375e-01,\n",
       "          1.45605266e-01],\n",
       "       [ -2.11865245e-03,   2.86232517e-03,   1.20671550e-02,\n",
       "         -1.43242916e-02,   9.71021783e-03,  -7.62111740e-03,\n",
       "          4.87436820e-03,   1.14759114e-02,   5.33796754e-03,\n",
       "         -1.49162579e-03,  -3.75673883e-02,   5.92554919e-03,\n",
       "         -1.76168291e-03,   1.58950947e-02,   1.13939866e-02,\n",
       "         -2.65078079e-02,   1.40006058e-02,   2.03428902e-02,\n",
       "          1.85709074e-02,   2.70684604e-02,  -1.62380189e-02,\n",
       "          4.21135463e-02,  -2.67949253e-02,  -2.78465287e-03,\n",
       "         -2.19002180e-02,   2.32481919e-02,   9.95759945e-03,\n",
       "         -4.76124603e-03,  -2.20224392e-02,  -4.09305654e-03,\n",
       "          9.20253713e-03,   1.78505555e-02,   2.13676393e-02,\n",
       "          7.61091663e-03,   2.68082116e-02,   2.90790969e-03,\n",
       "         -8.36035889e-03,  -9.19583067e-03,  -7.53136759e-04,\n",
       "         -3.18117510e-03,  -9.77969076e-03,   2.93046236e-02,\n",
       "          1.07574128e-02,  -3.55269127e-02,  -1.21853845e-02,\n",
       "          2.59933826e-02,  -4.06602281e-04,  -1.58039927e-02,\n",
       "         -4.78776498e-03,   6.26865122e-03,  -2.40332391e-02,\n",
       "          2.53396691e-04],\n",
       "       [  1.28390989e-03,   5.82435019e-02,  -2.37431154e-02,\n",
       "          6.87637478e-02,  -1.01030730e-01,  -4.44966517e-02,\n",
       "          1.68096684e-02,   4.05793749e-02,  -7.96478614e-02,\n",
       "         -1.43934302e-02,  -7.04732984e-02,  -1.53344339e-02,\n",
       "         -2.16084458e-02,   9.71632749e-02,   1.39518576e-02,\n",
       "         -1.60605870e-02,   1.24800596e-02,   1.33107631e-02,\n",
       "          1.37886051e-02,   1.78493895e-02,  -8.37455038e-03,\n",
       "          9.71259549e-03,   5.14683723e-02,  -4.51286398e-02,\n",
       "          2.80023795e-02,   2.20411755e-02,   3.45601626e-02,\n",
       "         -5.11583500e-02,   1.19684525e-02,  -6.94378167e-02,\n",
       "         -8.93385708e-02,   4.65504313e-03,  -7.26891011e-02,\n",
       "          3.28756869e-02,  -4.63420060e-03,  -2.09531630e-03,\n",
       "         -5.31409727e-03,   2.82961484e-02,  -1.15823711e-03,\n",
       "         -7.04215840e-03,  -3.68719213e-02,   4.82731909e-02,\n",
       "         -4.35974635e-02,   1.37164220e-02,  -2.49349698e-02,\n",
       "         -2.02480983e-03,  -2.27367543e-02,   1.13028521e-02,\n",
       "          6.57399520e-02,   6.53093755e-02,  -3.32388021e-02,\n",
       "          2.29721721e-02],\n",
       "       [  9.05813649e-04,   4.12079096e-02,  -1.68358739e-02,\n",
       "          4.86622080e-02,  -7.15187863e-02,  -3.15023027e-02,\n",
       "          1.19074546e-02,   2.87323035e-02,  -5.64238913e-02,\n",
       "         -1.02158841e-02,  -4.99268658e-02,  -1.08542610e-02,\n",
       "         -1.53071322e-02,   6.88001662e-02,   9.86602809e-03,\n",
       "         -1.13522848e-02,   8.84468015e-03,   9.43540875e-03,\n",
       "          9.75722726e-03,   1.26539804e-02,  -5.93989994e-03,\n",
       "          6.86734263e-03,   3.64268646e-02,  -3.19729708e-02,\n",
       "          1.97987072e-02,   1.55709498e-02,   2.44364254e-02,\n",
       "         -3.62224579e-02,   8.47623032e-03,  -4.91716079e-02,\n",
       "         -6.32504225e-02,   3.28787812e-03,  -5.14700152e-02,\n",
       "          2.32703015e-02,  -3.27610527e-03,  -1.50923256e-03,\n",
       "         -3.75103299e-03,   2.00245567e-02,  -8.32287944e-04,\n",
       "         -5.01110638e-03,  -2.61217430e-02,   3.41677144e-02,\n",
       "         -3.08716595e-02,   9.72491410e-03,  -1.76495407e-02,\n",
       "         -1.42301642e-03,  -1.60944387e-02,   7.99031090e-03,\n",
       "          4.65380512e-02,   4.62415069e-02,  -2.35034823e-02,\n",
       "          1.62754785e-02],\n",
       "       [ -1.31974490e-02,   1.75765213e-02,   8.73939767e-02,\n",
       "         -1.00707687e-01,   6.91914335e-02,  -5.28385118e-02,\n",
       "          3.66285257e-02,   7.82076344e-02,   3.85018140e-02,\n",
       "         -9.24488343e-03,  -2.67970413e-01,   4.10282128e-02,\n",
       "         -1.43135553e-02,   1.13997258e-01,   8.78507644e-02,\n",
       "         -1.91175252e-01,   1.01042390e-01,   1.46882340e-01,\n",
       "          1.28950030e-01,   1.92178041e-01,  -1.12090394e-01,\n",
       "          3.01908642e-01,  -1.89685509e-01,  -1.39356796e-02,\n",
       "         -1.53915226e-01,   1.67617962e-01,   6.88618720e-02,\n",
       "         -3.03687062e-02,  -1.58160970e-01,  -3.02030332e-02,\n",
       "          6.22093268e-02,   1.28969774e-01,   1.54792219e-01,\n",
       "          5.47349267e-02,   1.89444661e-01,   1.99679397e-02,\n",
       "         -6.05585314e-02,  -6.26637489e-02,  -3.88219976e-03,\n",
       "         -2.14874353e-02,  -6.71389550e-02,   2.07523048e-01,\n",
       "          7.38099962e-02,  -2.57379025e-01,  -8.58175755e-02,\n",
       "          1.83490857e-01,  -3.80190235e-04,  -1.14072360e-01,\n",
       "         -2.71802265e-02,   4.70047854e-02,  -1.69491544e-01,\n",
       "          1.07984839e-03]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.predict(np.random.normal(size=(5,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.predict(np.random.normal(size=(20,len(finalFeatures))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.predict(G.predict(np.random.normal(size=20)))\n",
    "#A.predict(np.random.normal(size=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.predict(G.predict(np.random.normal(size=(10,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.predict(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_n=0\n",
    "print(finalFeatures[feature_n])\n",
    "sns.distplot(G.predict(np.random.normal(size=(100000,1)))[:,feature_n], label=finalFeatures[feature_n])\n",
    "sns.distplot(Xtest[:,feature_n], label=finalFeatures[feature_n])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "if cvTests:\n",
    "    for i, history in enumerate(histories):\n",
    "        if i == 0:\n",
    "            plt.plot(history['loss'], color='g', label='Training')\n",
    "            plt.plot(history['val_loss'], color='b', label='Testing')\n",
    "        else:\n",
    "            plt.plot(history['loss'], color='g')\n",
    "            plt.plot(history['val_loss'], color='b')\n",
    "    plt.legend(fontsize=16)\n",
    "else:\n",
    "    for history in histories:\n",
    "        plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"Epoch\", fontsize=24, color='black')\n",
    "plt.ylabel(\"MSE\", fontsize=24, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"weights/NN_B_Regressor_App_\" + mode + \"_\" \n",
    "print (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added 3 'b's (?) for some reason some need and other don't... don't get it\n",
    "# i guess json as str and the others are bin\n",
    "\n",
    "os.system(\"rm \" + name + \"*.json\")\n",
    "os.system(\"rm \" + name + \"*.h5\")\n",
    "os.system(\"rm \" + name + \"*.pkl\")\n",
    "for i, model in enumerate(ensemble):\n",
    "    json_string = model.to_json()\n",
    "    open(name + '_' + str(i) + '.json', 'w').write(json_string) \n",
    "    model.save_weights(name + '_' + str(i) + '.h5')\n",
    "with open(name + '_compile.json', 'w') as fout:\n",
    "    json.dump(compileArgs, fout)\n",
    "with open(name + '_weights.pkl', 'wb') as fout:\n",
    "    pickle.dump(weights, fout)\n",
    "with open(name + '_inputPipe.pkl', 'wb') as fout:\n",
    "    pickle.dump(inputPipe, fout)\n",
    "with open(name + '_outputPipe.pkl', 'wb') as fout:\n",
    "    pickle.dump(outputPipe, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response of ensemble on development data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "weights = None\n",
    "inputPipe = None\n",
    "outputPipe = None\n",
    "compileArgs = None\n",
    "with open(name + '_compile.json', 'r') as fin:\n",
    "    compileArgs = json.load(fin)\n",
    "for i in range(ensembleSize):\n",
    "    model = model_from_json(open(name + '_' + str(i) + '.json').read())\n",
    "    model.load_weights(name + \"_\" + str(i) + '.h5')\n",
    "    model.compile(**compileArgs)\n",
    "    ensemble.append(model)\n",
    "with open(name + '_weights.pkl', 'rb') as fin:\n",
    "    weights = pickle.load(fin)\n",
    "with open(name + '_inputPipe.pkl', 'rb') as fin:\n",
    "    inputPipe = pickle.load(fin)\n",
    "with open(name + '_outputPipe.pkl', 'rb') as fin:\n",
    "    outputPipe = pickle.load(fin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
